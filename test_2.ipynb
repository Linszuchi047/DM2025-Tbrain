{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e6fc3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded datasets.\n",
      "[OK] Feature engineering completed. Accounts: 1800106; Features: 75\n",
      "[OK] Split -> Train accounts: 328988 (pos=1004, neg=327984); Test accounts: 4780\n",
      "--- Start LightGBM Stratified K-Fold Training ---\n",
      "Fold 1 finished. AUC: 0.9649\n",
      "Fold 2 finished. AUC: 0.9715\n",
      "Fold 3 finished. AUC: 0.9465\n",
      "Fold 4 finished. AUC: 0.9735\n",
      "Fold 5 finished. AUC: 0.9572\n",
      "\n",
      "[OOF] Total AUC=0.9600 üéØ F1@best=0.5107 üß™ Threshold=0.9891\n",
      "[OOF] Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9982    0.9992    0.9987    327984\n",
      "           1     0.6346    0.4273    0.5107      1004\n",
      "\n",
      "    accuracy                         0.9975    328988\n",
      "   macro avg     0.8164    0.7133    0.7547    328988\n",
      "weighted avg     0.9971    0.9975    0.9973    328988\n",
      "\n",
      "[ERROR] Âü∑Ë°åÈÅéÁ®ã‰∏≠ÁôºÁîüÈåØË™§: \"['is_esun_from', 'is_esun_to', 'is_esun'] not in index\"\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TransactionAlertPro_Optimized.py\n",
    "Optimized Solution for 2025 Esun AI Challenge (Binary Classification)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, average_precision_score, roc_auc_score, classification_report\n",
    "import lightgbm as lgb # ÂºïÂÖ• LightGBM\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 1. Utilities (ËºîÂä©ÂáΩÊï∏ - Â∑≤‰øÆÊ≠£ _safe_div)\n",
    "# ======================================================================\n",
    "\n",
    "def _to_numeric(s):\n",
    "    \"\"\"Best-effort numeric conversion; returns float (NaN if fail).\"\"\"\n",
    "    try:\n",
    "        return pd.to_numeric(s, errors=\"coerce\")\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _to_datetime(s):\n",
    "    \"\"\"Best-effort datetime parsing.\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    \"\"\"\n",
    "    Safe division that works for scalars, numpy arrays, and pandas Series/DataFrames.\n",
    "    Returns 0.0 where b is NaN or 0.\n",
    "    (FIXED: Ensure Pandas Series output when input is Pandas Series)\n",
    "    \"\"\"\n",
    "    if isinstance(a, (pd.Series, pd.DataFrame)) or isinstance(b, (pd.Series, pd.DataFrame)):\n",
    "        mask = pd.notna(b) & (b != 0)\n",
    "        \n",
    "        result_array = np.where(mask, a / b, 0.0)\n",
    "\n",
    "        if isinstance(a, pd.Series):\n",
    "            return pd.Series(result_array, index=a.index)\n",
    "        elif isinstance(b, pd.Series):\n",
    "            return pd.Series(result_array, index=b.index)\n",
    "        else:\n",
    "            return result_array\n",
    "    else:\n",
    "        return (a / b) if (b is not None and b != 0) else 0.0\n",
    "\n",
    "def _map_is_esun(v):\n",
    "    \"\"\"Map acct_type to 1 (esun) / 0 (others).\"\"\"\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    s = str(v).strip()\n",
    "    return 1 if s in {\"1\", \"01\", \"esun\", \"ESUN\", \"ÁéâÂ±±\"} else 0\n",
    "\n",
    "def _ensure_int(x):\n",
    "    \"\"\"Convert value to integer, return NaN on failure.\"\"\"\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _to_hour(v):\n",
    "    \"\"\"Best-effort extraction of hour (0-23) from time string.\"\"\"\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    s = str(v).strip()\n",
    "    s2 = ''.join(ch for ch in s if (ch.isdigit() or ch == ':'))\n",
    "    if ':' in s2:\n",
    "        try:\n",
    "            hh = int(s2.split(':')[0])\n",
    "            return hh if 0 <= hh <= 23 else np.nan\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    try:\n",
    "        s2 = s2.zfill(6)\n",
    "        hh = int(s2[:2])\n",
    "        return hh if 0 <= hh <= 23 else np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# ======================================================================\n",
    "# 2. Data Loading\n",
    "# ======================================================================\n",
    "def load_csvs(dir_path: str):\n",
    "    df_txn = pd.read_csv(os.path.join(dir_path, 'acct_transaction.csv'))\n",
    "    df_alert = pd.read_csv(os.path.join(dir_path, 'acct_alert.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(dir_path, 'acct_predict.csv'))\n",
    "    print(\"[OK] Loaded datasets.\")\n",
    "    return df_txn, df_alert, df_test\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 3. Feature Engineering (Enhanced)\n",
    "# ======================================================================\n",
    "def engineer_features_enhanced(df_txn: pd.DataFrame, df_alert: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build robust account-level features using both sender and receiver roles.\n",
    "    [ENHANCED]: Log amounts, time-of-day bins, and risk propagation features.\n",
    "    \"\"\"\n",
    "    df = df_txn.copy()\n",
    "    \n",
    "    # --- Robust typing & Pre-processing ---\n",
    "    if 'txn_amt' in df.columns:\n",
    "        df['txn_amt'] = df['txn_amt'].apply(_to_numeric)\n",
    "        df['txn_amt_log'] = np.log1p(df['txn_amt'])\n",
    "    else:\n",
    "        raise ValueError(\"Column 'txn_amt' not found.\")\n",
    "\n",
    "    if 'txn_date' in df.columns:\n",
    "        df['txn_date_int'] = df['txn_date'].apply(_ensure_int) \n",
    "    else:\n",
    "        df['txn_date_int'] = np.nan\n",
    "\n",
    "    if 'txn_time' in df.columns:\n",
    "        df['hour'] = df['txn_time'].apply(_to_hour)\n",
    "    else:\n",
    "        df['hour'] = np.nan\n",
    "\n",
    "    for col in ['from_acct_type', 'to_acct_type']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(_map_is_esun)\n",
    "        else:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    if 'is_self_txn' in df.columns:\n",
    "        df['is_self_txn_f'] = df['is_self_txn'].map(lambda x: 1 if str(x).strip().upper() == 'Y' else (0 if str(x).strip().upper() == 'N' else np.nan))\n",
    "    else:\n",
    "        df['is_self_txn_f'] = np.nan\n",
    "\n",
    "    if 'channel_type' in df.columns:\n",
    "        df['channel_type_norm'] = df['channel_type'].astype(str).str.strip().fillna('UNK')\n",
    "    else:\n",
    "        df['channel_type_norm'] = 'UNK'\n",
    "\n",
    "    if 'currency_type' in df.columns:\n",
    "        df['currency_type_norm'] = df['currency_type'].astype(str).str.strip().fillna('UNK')\n",
    "    else:\n",
    "        df['currency_type_norm'] = 'UNK'\n",
    "\n",
    "    if not set(['from_acct', 'to_acct']).issubset(df.columns):\n",
    "        raise ValueError(\"from_acct and to_acct must exist.\")\n",
    "\n",
    "    # --- NEW: Time-of-Day Bins ---\n",
    "    def _time_bin(h):\n",
    "        if pd.isna(h): return 'UNK'\n",
    "        if 6 <= h < 12: return 'MORN'\n",
    "        elif 12 <= h < 18: return 'AFTN'\n",
    "        elif 18 <= h < 22: return 'EVNG'\n",
    "        else: return 'NGHT'\n",
    "        \n",
    "    df['time_bin'] = df['hour'].apply(_time_bin)\n",
    "\n",
    "    # --- NEW: Risk Propagation Feature ---\n",
    "    alert_accts = set(df_alert['acct'].astype(str))\n",
    "    df['to_is_alert'] = df['to_acct'].astype(str).isin(alert_accts).astype(int)\n",
    "    df['from_is_alert'] = df['from_acct'].astype(str).isin(alert_accts).astype(int)\n",
    "    \n",
    "    \n",
    "    # --- per-account aggregates (sender side) ---\n",
    "    g_from = df.groupby('from_acct')\n",
    "    send_cnt = g_from.size().rename('send_cnt')\n",
    "    \n",
    "    # Amount Stats (Enhanced: Log, TWD/USD Sums)\n",
    "    send_amt_agg = g_from['txn_amt'].agg(['sum', 'mean', 'std', 'max', 'min', 'median']).add_prefix('send_amt_')\n",
    "    send_amt_log_agg = g_from['txn_amt_log'].agg(['mean', 'std']).add_prefix('send_amt_log_')\n",
    "    send_twd_sum = g_from.apply(lambda x: x[x['currency_type_norm'] == 'TWD']['txn_amt'].sum()).rename('send_twd_sum')\n",
    "    send_usd_sum = g_from.apply(lambda x: x[x['currency_type_norm'] == 'USD']['txn_amt'].sum()).rename('send_usd_sum')\n",
    "    \n",
    "    # Activity & Frequency\n",
    "    send_active_days = g_from['txn_date_int'].nunique().rename('send_active_days')\n",
    "    send_span = (g_from['txn_date_int'].max() - g_from['txn_date_int'].min()).replace(0, 1).rename('send_span')\n",
    "    # ERROR LINE FIXED HERE: _safe_div now returns Series\n",
    "    send_freq_per_day = _safe_div(send_cnt, send_span).rename('send_freq_per_day') \n",
    "    \n",
    "    # Behavior & Category Stats\n",
    "    send_unique_ctp = g_from['to_acct'].nunique().rename('send_unique_ctp')\n",
    "    send_hour_agg = g_from['hour'].agg(['mean', 'std']).add_prefix('send_hour_')\n",
    "    send_self_agg = g_from['is_self_txn_f'].agg(['sum', 'mean']).rename(index={'sum': 'send_self_cnt', 'mean': 'send_self_ratio'})\n",
    "    send_to_esun_ratio = g_from['to_acct_type'].mean().rename('send_to_esun_ratio')\n",
    "    \n",
    "    # NEW: Risk Propagation Ratio\n",
    "    send_to_alert_ratio = _safe_div(g_from['to_is_alert'].sum(), send_cnt).rename('send_to_alert_ratio')\n",
    "\n",
    "    # Channel One-Hot Proportions\n",
    "    top_channels = df['channel_type_norm'].value_counts().head(8).index.tolist()\n",
    "    send_channel_props = [\n",
    "        g_from.apply(lambda s, ch=ch: (s['channel_type_norm']==ch).mean()).rename(f'send_ch_{ch}_prop')\n",
    "        for ch in top_channels\n",
    "    ]\n",
    "    \n",
    "    # Merge sender features\n",
    "    left = (\n",
    "        pd.concat([\n",
    "            send_cnt, send_amt_agg, send_amt_log_agg, send_twd_sum, send_usd_sum,\n",
    "            send_active_days, send_span, send_freq_per_day, send_unique_ctp,\n",
    "            send_hour_agg, send_self_agg, send_to_esun_ratio, send_to_alert_ratio,\n",
    "            g_from['currency_type_norm'].nunique().rename('send_curr_nunique'), \n",
    "            g_from.apply(lambda s: (s['currency_type_norm']=='TWD').mean()).rename('send_curr_twd_ratio'),\n",
    "        ] + send_channel_props, axis=1)\n",
    "        .reset_index().rename(columns={'from_acct': 'acct'})\n",
    "    )\n",
    "\n",
    "    # --- per-account aggregates (receiver side) ---\n",
    "    g_to = df.groupby('to_acct')\n",
    "    recv_cnt = g_to.size().rename('recv_cnt')\n",
    "    \n",
    "    # Amount Stats (Enhanced: Log, TWD/USD Sums)\n",
    "    recv_amt_agg = g_to['txn_amt'].agg(['sum', 'mean', 'std', 'max', 'min', 'median']).add_prefix('recv_amt_')\n",
    "    recv_amt_log_agg = g_to['txn_amt_log'].agg(['mean', 'std']).add_prefix('recv_amt_log_')\n",
    "    recv_twd_sum = g_to.apply(lambda x: x[x['currency_type_norm'] == 'TWD']['txn_amt'].sum()).rename('recv_twd_sum')\n",
    "    recv_usd_sum = g_to.apply(lambda x: x[x['currency_type_norm'] == 'USD']['txn_amt'].sum()).rename('recv_usd_sum')\n",
    "    \n",
    "    # Activity & Frequency\n",
    "    recv_active_days = g_to['txn_date_int'].nunique().rename('recv_active_days')\n",
    "    recv_span = (g_to['txn_date_int'].max() - g_to['txn_date_int'].min()).replace(0, 1).rename('recv_span')\n",
    "    # ERROR LINE FIXED HERE: _safe_div now returns Series\n",
    "    recv_freq_per_day = _safe_div(recv_cnt, recv_span).rename('recv_freq_per_day')\n",
    "\n",
    "    # Behavior & Category Stats\n",
    "    recv_unique_ctp = g_to['from_acct'].nunique().rename('recv_unique_ctp')\n",
    "    recv_hour_agg = g_to['hour'].agg(['mean', 'std']).add_prefix('recv_hour_')\n",
    "    recv_self_agg = g_to['is_self_txn_f'].agg(['sum', 'mean']).rename(index={'sum': 'recv_self_cnt', 'mean': 'recv_self_ratio'})\n",
    "    recv_from_esun_ratio = g_to['from_acct_type'].mean().rename('recv_from_esun_ratio')\n",
    "    \n",
    "    # NEW: Risk Propagation Ratio\n",
    "    recv_from_alert_ratio = _safe_div(g_to['from_is_alert'].sum(), recv_cnt).rename('recv_from_alert_ratio')\n",
    "\n",
    "    # Channel One-Hot Proportions\n",
    "    recv_channel_props = [\n",
    "        g_to.apply(lambda s, ch=ch: (s['channel_type_norm']==ch).mean()).rename(f'recv_ch_{ch}_prop')\n",
    "        for ch in top_channels\n",
    "    ]\n",
    "\n",
    "    # Merge receiver features\n",
    "    right = (\n",
    "        pd.concat([\n",
    "            recv_cnt, recv_amt_agg, recv_amt_log_agg, recv_twd_sum, recv_usd_sum,\n",
    "            recv_active_days, recv_span, recv_freq_per_day, recv_unique_ctp,\n",
    "            recv_hour_agg, recv_self_agg, recv_from_esun_ratio, recv_from_alert_ratio,\n",
    "            g_to['currency_type_norm'].nunique().rename('recv_curr_nunique'), \n",
    "            g_to.apply(lambda s: (s['currency_type_norm']=='TWD').mean()).rename('recv_curr_twd_ratio'),\n",
    "        ] + recv_channel_props, axis=1)\n",
    "        .reset_index().rename(columns={'to_acct': 'acct'})\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- graph-like reciprocity ---\n",
    "    out_sets = df.groupby('from_acct')['to_acct'].apply(set)\n",
    "    in_sets = df.groupby('to_acct')['from_acct'].apply(set)\n",
    "    all_accts = set(out_sets.index).union(set(in_sets.index))\n",
    "\n",
    "    reci_ratio = {}\n",
    "    total_degree = {}\n",
    "    bi_degree = {}\n",
    "\n",
    "    for a in all_accts:\n",
    "        outs = out_sets.get(a, set())\n",
    "        ins = in_sets.get(a, set())\n",
    "        deg = len(outs.union(ins))\n",
    "        bi = len(outs.intersection(ins))\n",
    "        total_degree[a] = deg\n",
    "        bi_degree[a] = bi\n",
    "        reci_ratio[a] = _safe_div(bi, deg)\n",
    "\n",
    "    df_graph = pd.DataFrame({\n",
    "        'acct': list(all_accts),\n",
    "        'graph_degree': [total_degree[a] for a in all_accts],\n",
    "        'graph_bi_degree': [bi_degree[a] for a in all_accts],\n",
    "        'graph_reciprocity': [reci_ratio[a] for a in all_accts],\n",
    "    })\n",
    "\n",
    "    # --- merge all features to account level ---\n",
    "    feat = pd.merge(left, right, on='acct', how='outer')\n",
    "    feat = feat.merge(df_graph, on='acct', how='left')\n",
    "\n",
    "    # Add simple totals / balances\n",
    "    feat['total_amt_sum'] = feat['send_amt_sum'].fillna(0) + feat['recv_amt_sum'].fillna(0)\n",
    "    feat['net_out_amt'] = feat['send_amt_sum'].fillna(0) - feat['recv_amt_sum'].fillna(0)\n",
    "    feat['total_cnt'] = feat['send_cnt'].fillna(0) + feat['recv_cnt'].fillna(0)\n",
    "\n",
    "    # Normalize some ratios\n",
    "    feat['send_avg_amt'] = _safe_div(feat['send_amt_sum'].fillna(0), feat['send_cnt'].replace(0, np.nan))\n",
    "    feat['recv_avg_amt'] = _safe_div(feat['recv_amt_sum'].fillna(0), feat['recv_cnt'].replace(0, np.nan))\n",
    "    feat['send_unique_rate'] = _safe_div(feat['send_unique_ctp'].fillna(0), feat['send_cnt'].replace(0, np.nan))\n",
    "    feat['recv_unique_rate'] = _safe_div(feat['recv_unique_ctp'].fillna(0), feat['recv_cnt'].replace(0, np.nan))\n",
    "\n",
    "    # Determine account type (is_esun)\n",
    "    df_from_type = df[['from_acct', 'from_acct_type']].drop_duplicates().rename(columns={'from_acct': 'acct', 'from_acct_type': 'is_esun_from'})\n",
    "    df_to_type = df[['to_acct', 'to_acct_type']].drop_duplicates().rename(columns={'to_acct': 'acct', 'to_acct_type': 'is_esun_to'})\n",
    "    feat = feat.merge(df_from_type, on='acct', how='left').merge(df_to_type, on='acct', how='left')\n",
    "    feat['is_esun'] = feat[['is_esun_from', 'is_esun_to']].max(axis=1).fillna(0)\n",
    "\n",
    "    # Fill NaNs with 0 for model; keep acct id\n",
    "    feat = feat.fillna(0)\n",
    "    print(f\"[OK] Feature engineering completed. Accounts: {len(feat)}; Features: {feat.shape[1]-1}\")\n",
    "    return feat\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 4. Train / Test Split\n",
    "# ======================================================================\n",
    "def make_splits(feat_df: pd.DataFrame, df_alert: pd.DataFrame, df_test: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build training labels and test set.\n",
    "    \"\"\"\n",
    "    feat = feat_df.copy()\n",
    "\n",
    "    # Labels\n",
    "    alert_set = set(df_alert['acct'].astype(str))\n",
    "    feat['label'] = feat['acct'].astype(str).isin(alert_set).astype(int)\n",
    "\n",
    "    # Test mask\n",
    "    test_set = set(df_test['acct'].astype(str))\n",
    "\n",
    "    # Keep only esun in train & exclude test set\n",
    "    train_df = feat[(~feat['acct'].astype(str).isin(test_set)) & (feat['is_esun'] == 1)].copy()\n",
    "    X = train_df.drop(columns=['label'])\n",
    "    y = train_df['label'].values\n",
    "\n",
    "    # Test data: exactly test acct list joined with features (missing -> 0)\n",
    "    test_feat = feat[feat['acct'].astype(str).isin(test_set)].copy()\n",
    "    X_test = df_test[['acct']].merge(test_feat.drop(columns=['label', 'is_esun_from', 'is_esun_to'], errors='ignore'), on='acct', how='left').fillna(0)\n",
    "    X_test = X_test.merge(feat[['acct', 'is_esun']], on='acct', how='left').fillna(0)\n",
    "\n",
    "    print(f\"[OK] Split -> Train accounts: {len(X)} (pos={sum(y)}, neg={len(y)-sum(y)}); Test accounts: {len(X_test)}\")\n",
    "    return X, y, X_test\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 5. Modeling (LightGBM + K-Fold)\n",
    "# ======================================================================\n",
    "\n",
    "def fit_model_kfold(X: pd.DataFrame, y: np.ndarray, random_state: int = 42, n_splits: int = 5):\n",
    "    \"\"\"\n",
    "    Train LightGBM using Stratified K-Fold and tune decision threshold on OOF predictions.\n",
    "    \"\"\"\n",
    "    drop_cols = ['acct']\n",
    "    feat_cols = [c for c in X.columns if c not in drop_cols]\n",
    "    \n",
    "    # LightGBM requires feature names without certain characters (<, >, :, =, [, ])\n",
    "    X_features = X[feat_cols].copy() \n",
    "    \n",
    "    safe_feat_cols = [c.replace('[', '_').replace(']', '_').replace('<', '_').replace('>', '_').replace(':', '_').replace('=', '_') for c in feat_cols]\n",
    "    col_name_map = dict(zip(feat_cols, safe_feat_cols))\n",
    "    \n",
    "    # ‰øÆÊ≠£: Âú® DataFrame ‰∏ä‰ΩøÁî® rename\n",
    "    X_safe = X_features.rename(columns=col_name_map)\n",
    "    \n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc', \n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': 6,\n",
    "        'seed': random_state,\n",
    "        'n_jobs': -1,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'subsample': 0.7,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'is_unbalance': True,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    oof_preds = np.zeros(len(X_safe))\n",
    "    trained_models = []\n",
    "\n",
    "    print(\"--- Start LightGBM Stratified K-Fold Training ---\")\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_safe, y)):\n",
    "        X_train, X_val = X_safe.iloc[train_index], X_safe.iloc[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        lgb_clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        \n",
    "        lgb_clf.fit(X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    eval_metric='auc',\n",
    "                    callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "        oof_preds[val_index] = lgb_clf.predict_proba(X_val)[:, 1]\n",
    "        trained_models.append(lgb_clf)\n",
    "        print(f\"Fold {fold+1} finished. AUC: {roc_auc_score(y_val, oof_preds[val_index]):.4f}\")\n",
    "\n",
    "    # Threshold tuning on OOF predictions\n",
    "    prec, rec, thr = precision_recall_curve(y, oof_preds)\n",
    "    f1s = 2 * prec * rec / (prec + rec + 1e-12) \n",
    "    \n",
    "    best_idx = np.nanargmax(f1s)\n",
    "    best_thr = thr[best_idx] if best_idx < len(thr) else 0.5\n",
    "    \n",
    "    y_oof_pred = (oof_preds >= best_thr).astype(int)\n",
    "    f1_oof = f1_score(y, y_oof_pred)\n",
    "\n",
    "    print(f\"\\n[OOF] Total AUC={roc_auc_score(y, oof_preds):.4f} üéØ F1@best={f1_oof:.4f} üß™ Threshold={best_thr:.4f}\")\n",
    "    print(\"[OOF] Classification report:\\n\", classification_report(y, y_oof_pred, digits=4))\n",
    "    \n",
    "    return trained_models, safe_feat_cols, best_thr\n",
    "\n",
    "\n",
    "def predict_test_kfold(trained_models: List, feat_cols: List[str], threshold: float, X_test: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Predict test data using K-Fold models (Averaging probabilities).\n",
    "    \"\"\"\n",
    "    X_test_safe = X_test.copy()\n",
    "    \n",
    "    original_feat_cols = [c.replace('_', '[', 1).replace('_', ']', 1).replace('_', ':') for c in feat_cols]\n",
    "    col_name_map = dict(zip(original_feat_cols, feat_cols))\n",
    "    \n",
    "    cols_to_rename = {k: v for k, v in col_name_map.items() if k in X_test_safe.columns}\n",
    "    X_test_safe = X_test_safe.rename(columns=cols_to_rename)\n",
    "\n",
    "\n",
    "    test_preds = np.zeros(len(X_test_safe))\n",
    "    for model in trained_models:\n",
    "        test_preds += model.predict_proba(X_test_safe[feat_cols])[:, 1]\n",
    "    \n",
    "    avg_proba = test_preds / len(trained_models)\n",
    "    y_pred = (avg_proba >= threshold).astype(int)\n",
    "    return y_pred, avg_proba\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 6. Output\n",
    "# ======================================================================\n",
    "def save_submission(path: str, df_test: pd.DataFrame, X_test: pd.DataFrame, y_pred: np.ndarray):\n",
    "    \"\"\"Saves the final prediction result in the required format.\"\"\"\n",
    "    df_pred = pd.DataFrame({\n",
    "        'acct': X_test['acct'].values,\n",
    "        'label': y_pred.astype(int)\n",
    "    })\n",
    "    out = df_test[['acct']].merge(df_pred, on='acct', how='left').fillna(0)\n",
    "    out.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK] Saved submission to: {path}\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 7. Main\n",
    "# ======================================================================\n",
    "def main():\n",
    "    # === change this to your data directory ===\n",
    "    dir_path = \"data\" \n",
    "\n",
    "    try:\n",
    "        # 1. Ë≥áÊñôËºâÂÖ•\n",
    "        df_txn, df_alert, df_test = load_csvs(dir_path)\n",
    "        \n",
    "        # 2. Â¢ûÂº∑ÁâπÂæµÂ∑•Á®ã\n",
    "        feat_df = engineer_features_enhanced(df_txn, df_alert)\n",
    "        \n",
    "        # 3. Êï∏ÊìöÂàáÂàÜ\n",
    "        X, y, X_test = make_splits(feat_df, df_alert, df_test)\n",
    "        \n",
    "        # 4. LightGBM K-Fold Ë®ìÁ∑¥ËàáÈñÄÊ™ªË™øÂÑ™\n",
    "        # üö® Ë≠¶Âëä: Ê≠§Ê≠•È©üÈúÄË¶Å lightgbm ÂáΩÂºèÂ∫´\n",
    "        trained_models, feat_cols, thr = fit_model_kfold(X, y, random_state=42, n_splits=5)\n",
    "        \n",
    "        # 5. K-Fold È†êÊ∏¨\n",
    "        y_pred, _ = predict_test_kfold(trained_models, feat_cols, thr, X_test)\n",
    "        \n",
    "        # 6. ÂÑ≤Â≠òÁµêÊûú\n",
    "        save_submission(\"enhanced_result.csv\", df_test, X_test, y_pred)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"[ERROR] Ê™îÊ°àÊâæ‰∏çÂà∞Ôºö{e}. Ë´ãÁ¢∫‰øùÊÇ®ÁöÑ CSV Ê™îÊ°àÂ≠òÊîæÂú® '{dir_path}' Ë≥áÊñôÂ§æ‰∏≠Ôºå‰∏îÊ™îÂêçÊ≠£Á¢∫„ÄÇ\")\n",
    "    except ImportError:\n",
    "        print(\"\\nüö® ERROR: Êâæ‰∏çÂà∞ lightgbm ÂáΩÂºèÂ∫´„ÄÇË´ãÂü∑Ë°å pip install lightgbm\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Âü∑Ë°åÈÅéÁ®ã‰∏≠ÁôºÁîüÈåØË™§: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "188903bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded datasets.\n",
      "[OK] Feature engineering completed. Accounts: 1800106; Features: 75\n",
      "[OK] Split -> Train accounts: 328988 (pos=1004, neg=327984); Test accounts: 4780\n",
      "--- Start LightGBM Stratified K-Fold Training ---\n",
      "Fold 1 finished. AUC: 0.9645\n",
      "Fold 2 finished. AUC: 0.9471\n",
      "Fold 3 finished. AUC: 0.9530\n",
      "Fold 4 finished. AUC: 0.9657\n",
      "Fold 5 finished. AUC: 0.9713\n",
      "\n",
      "[OOF] Total AUC=0.9571 üéØ F1@best=0.4974 üß™ Threshold=0.9804\n",
      "[OOF] Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9984    0.9986    0.9985    327984\n",
      "           1     0.5184    0.4781    0.4974      1004\n",
      "\n",
      "    accuracy                         0.9971    328988\n",
      "   macro avg     0.7584    0.7384    0.7480    328988\n",
      "weighted avg     0.9969    0.9971    0.9970    328988\n",
      "\n",
      "[OK] Saved submission to: enhanced_result.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TransactionAlertPro_Optimized_Fixed.py\n",
    "Optimized Solution for 2025 Esun AI Challenge (Binary Classification)\n",
    "- Fixes the KeyError by ensuring train/test feature parity\n",
    "- Cleans LightGBM feature-name sanitization (consistent rename map)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, roc_auc_score, classification_report\n",
    "import lightgbm as lgb  # pip install lightgbm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 1. Utilities\n",
    "# ======================================================================\n",
    "\n",
    "def _to_numeric(s):\n",
    "    try:\n",
    "        return pd.to_numeric(s, errors=\"coerce\")\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _ensure_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _to_hour(v):\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    s = str(v).strip()\n",
    "    s2 = ''.join(ch for ch in s if (ch.isdigit() or ch == ':'))\n",
    "    if ':' in s2:\n",
    "        try:\n",
    "            hh = int(s2.split(':')[0])\n",
    "            return hh if 0 <= hh <= 23 else np.nan\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    try:\n",
    "        s2 = s2.zfill(6)\n",
    "        hh = int(s2[:2])\n",
    "        return hh if 0 <= hh <= 23 else np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    \"\"\"Safe division: works for scalars/Series/DataFrames; returns 0.0 where b is NaN or 0.\"\"\"\n",
    "    if isinstance(a, (pd.Series, pd.DataFrame)) or isinstance(b, (pd.Series, pd.DataFrame)):\n",
    "        mask = pd.notna(b) & (b != 0)\n",
    "        result_array = np.where(mask, a / b, 0.0)\n",
    "        if isinstance(a, pd.Series):\n",
    "            return pd.Series(result_array, index=a.index)\n",
    "        elif isinstance(b, pd.Series):\n",
    "            return pd.Series(result_array, index=b.index)\n",
    "        else:\n",
    "            return result_array\n",
    "    else:\n",
    "        return (a / b) if (b is not None and b != 0) else 0.0\n",
    "\n",
    "def _map_is_esun(v):\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    s = str(v).strip()\n",
    "    return 1 if s in {\"1\", \"01\", \"esun\", \"ESUN\", \"ÁéâÂ±±\"} else 0\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 2. Data Loading\n",
    "# ======================================================================\n",
    "def load_csvs(dir_path: str):\n",
    "    df_txn = pd.read_csv(os.path.join(dir_path, 'acct_transaction.csv'))\n",
    "    df_alert = pd.read_csv(os.path.join(dir_path, 'acct_alert.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(dir_path, 'acct_predict.csv'))\n",
    "    print(\"[OK] Loaded datasets.\")\n",
    "    return df_txn, df_alert, df_test\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 3. Feature Engineering (Enhanced)\n",
    "# ======================================================================\n",
    "def engineer_features_enhanced(df_txn: pd.DataFrame, df_alert: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build robust account-level features using both sender and receiver roles.\n",
    "    Includes: log-amounts, time-of-day bins, simple risk-propagation, graph features.\n",
    "    \"\"\"\n",
    "    df = df_txn.copy()\n",
    "\n",
    "    # --- Robust typing & Pre-processing ---\n",
    "    if 'txn_amt' not in df.columns:\n",
    "        raise ValueError(\"Column 'txn_amt' not found.\")\n",
    "    df['txn_amt'] = df['txn_amt'].apply(_to_numeric)\n",
    "    df['txn_amt_log'] = np.log1p(df['txn_amt'])\n",
    "\n",
    "    df['txn_date_int'] = df['txn_date'].apply(_ensure_int) if 'txn_date' in df.columns else np.nan\n",
    "    df['hour'] = df['txn_time'].apply(_to_hour) if 'txn_time' in df.columns else np.nan\n",
    "\n",
    "    for col in ['from_acct_type', 'to_acct_type']:\n",
    "        df[col] = df[col].apply(_map_is_esun) if col in df.columns else np.nan\n",
    "\n",
    "    if 'is_self_txn' in df.columns:\n",
    "        df['is_self_txn_f'] = df['is_self_txn'].map(\n",
    "            lambda x: 1 if str(x).strip().upper() == 'Y' else (0 if str(x).strip().upper() == 'N' else np.nan)\n",
    "        )\n",
    "    else:\n",
    "        df['is_self_txn_f'] = np.nan\n",
    "\n",
    "    df['channel_type_norm'] = df['channel_type'].astype(str).str.strip().fillna('UNK') if 'channel_type' in df.columns else 'UNK'\n",
    "    df['currency_type_norm'] = df['currency_type'].astype(str).str.strip().fillna('UNK') if 'currency_type' in df.columns else 'UNK'\n",
    "\n",
    "    if not set(['from_acct', 'to_acct']).issubset(df.columns):\n",
    "        raise ValueError(\"from_acct and to_acct must exist.\")\n",
    "\n",
    "    # --- Time-of-Day Bins ---\n",
    "    def _time_bin(h):\n",
    "        if pd.isna(h): return 'UNK'\n",
    "        if 6 <= h < 12: return 'MORN'\n",
    "        elif 12 <= h < 18: return 'AFTN'\n",
    "        elif 18 <= h < 22: return 'EVNG'\n",
    "        else: return 'NGHT'\n",
    "    df['time_bin'] = df['hour'].apply(_time_bin)\n",
    "\n",
    "    # --- Risk Propagation Feature ---\n",
    "    alert_accts = set(df_alert['acct'].astype(str))\n",
    "    df['to_is_alert'] = df['to_acct'].astype(str).isin(alert_accts).astype(int)\n",
    "    df['from_is_alert'] = df['from_acct'].astype(str).isin(alert_accts).astype(int)\n",
    "\n",
    "    # Sender side aggregates\n",
    "    g_from = df.groupby('from_acct')\n",
    "    send_cnt = g_from.size().rename('send_cnt')\n",
    "\n",
    "    send_amt_agg = g_from['txn_amt'].agg(['sum', 'mean', 'std', 'max', 'min', 'median']).add_prefix('send_amt_')\n",
    "    send_amt_log_agg = g_from['txn_amt_log'].agg(['mean', 'std']).add_prefix('send_amt_log_')\n",
    "    send_twd_sum = g_from.apply(lambda x: x[x['currency_type_norm'] == 'TWD']['txn_amt'].sum()).rename('send_twd_sum')\n",
    "    send_usd_sum = g_from.apply(lambda x: x[x['currency_type_norm'] == 'USD']['txn_amt'].sum()).rename('send_usd_sum')\n",
    "\n",
    "    send_active_days = g_from['txn_date_int'].nunique().rename('send_active_days')\n",
    "    send_span = (g_from['txn_date_int'].max() - g_from['txn_date_int'].min()).replace(0, 1).rename('send_span')\n",
    "    send_freq_per_day = _safe_div(send_cnt, send_span).rename('send_freq_per_day')\n",
    "\n",
    "    send_unique_ctp = g_from['to_acct'].nunique().rename('send_unique_ctp')\n",
    "    send_hour_agg = g_from['hour'].agg(['mean', 'std']).add_prefix('send_hour_')\n",
    "    send_self_agg = g_from['is_self_txn_f'].agg(['sum', 'mean']).rename(index={'sum': 'send_self_cnt', 'mean': 'send_self_ratio'})\n",
    "    send_to_esun_ratio = g_from['to_acct_type'].mean().rename('send_to_esun_ratio')\n",
    "    send_to_alert_ratio = _safe_div(g_from['to_is_alert'].sum(), send_cnt).rename('send_to_alert_ratio')\n",
    "\n",
    "    top_channels = df['channel_type_norm'].value_counts().head(8).index.tolist()\n",
    "    send_channel_props = [\n",
    "        g_from.apply(lambda s, ch=ch: (s['channel_type_norm'] == ch).mean()).rename(f'send_ch_{ch}_prop')\n",
    "        for ch in top_channels\n",
    "    ]\n",
    "\n",
    "    left = (\n",
    "        pd.concat([\n",
    "            send_cnt, send_amt_agg, send_amt_log_agg, send_twd_sum, send_usd_sum,\n",
    "            send_active_days, send_span, send_freq_per_day, send_unique_ctp,\n",
    "            send_hour_agg, send_self_agg, send_to_esun_ratio, send_to_alert_ratio,\n",
    "            g_from['currency_type_norm'].nunique().rename('send_curr_nunique'),\n",
    "            g_from.apply(lambda s: (s['currency_type_norm'] == 'TWD').mean()).rename('send_curr_twd_ratio'),\n",
    "        ] + send_channel_props, axis=1)\n",
    "        .reset_index().rename(columns={'from_acct': 'acct'})\n",
    "    )\n",
    "\n",
    "    # Receiver side aggregates\n",
    "    g_to = df.groupby('to_acct')\n",
    "    recv_cnt = g_to.size().rename('recv_cnt')\n",
    "\n",
    "    recv_amt_agg = g_to['txn_amt'].agg(['sum', 'mean', 'std', 'max', 'min', 'median']).add_prefix('recv_amt_')\n",
    "    recv_amt_log_agg = g_to['txn_amt_log'].agg(['mean', 'std']).add_prefix('recv_amt_log_')\n",
    "    recv_twd_sum = g_to.apply(lambda x: x[x['currency_type_norm'] == 'TWD']['txn_amt'].sum()).rename('recv_twd_sum')\n",
    "    recv_usd_sum = g_to.apply(lambda x: x[x['currency_type_norm'] == 'USD']['txn_amt'].sum()).rename('recv_usd_sum')\n",
    "\n",
    "    recv_active_days = g_to['txn_date_int'].nunique().rename('recv_active_days')\n",
    "    recv_span = (g_to['txn_date_int'].max() - g_to['txn_date_int'].min()).replace(0, 1).rename('recv_span')\n",
    "    recv_freq_per_day = _safe_div(recv_cnt, recv_span).rename('recv_freq_per_day')\n",
    "\n",
    "    recv_unique_ctp = g_to['from_acct'].nunique().rename('recv_unique_ctp')\n",
    "    recv_hour_agg = g_to['hour'].agg(['mean', 'std']).add_prefix('recv_hour_')\n",
    "    recv_self_agg = g_to['is_self_txn_f'].agg(['sum', 'mean']).rename(index={'sum': 'recv_self_cnt', 'mean': 'recv_self_ratio'})\n",
    "    recv_from_esun_ratio = g_to['from_acct_type'].mean().rename('recv_from_esun_ratio')\n",
    "    recv_from_alert_ratio = _safe_div(g_to['from_is_alert'].sum(), recv_cnt).rename('recv_from_alert_ratio')\n",
    "\n",
    "    recv_channel_props = [\n",
    "        g_to.apply(lambda s, ch=ch: (s['channel_type_norm'] == ch).mean()).rename(f'recv_ch_{ch}_prop')\n",
    "        for ch in top_channels\n",
    "    ]\n",
    "\n",
    "    right = (\n",
    "        pd.concat([\n",
    "            recv_cnt, recv_amt_agg, recv_amt_log_agg, recv_twd_sum, recv_usd_sum,\n",
    "            recv_active_days, recv_span, recv_freq_per_day, recv_unique_ctp,\n",
    "            recv_hour_agg, recv_self_agg, recv_from_esun_ratio, recv_from_alert_ratio,\n",
    "            g_to['currency_type_norm'].nunique().rename('recv_curr_nunique'),\n",
    "            g_to.apply(lambda s: (s['currency_type_norm'] == 'TWD').mean()).rename('recv_curr_twd_ratio'),\n",
    "        ] + recv_channel_props, axis=1)\n",
    "        .reset_index().rename(columns={'to_acct': 'acct'})\n",
    "    )\n",
    "\n",
    "    # Graph-like features\n",
    "    out_sets = df.groupby('from_acct')['to_acct'].apply(set)\n",
    "    in_sets = df.groupby('to_acct')['from_acct'].apply(set)\n",
    "    all_accts = set(out_sets.index).union(set(in_sets.index))\n",
    "\n",
    "    reci_ratio = {}\n",
    "    total_degree = {}\n",
    "    bi_degree = {}\n",
    "    for a in all_accts:\n",
    "        outs = out_sets.get(a, set())\n",
    "        ins = in_sets.get(a, set())\n",
    "        deg = len(outs.union(ins))\n",
    "        bi = len(outs.intersection(ins))\n",
    "        total_degree[a] = deg\n",
    "        bi_degree[a] = bi\n",
    "        reci_ratio[a] = _safe_div(bi, deg)\n",
    "\n",
    "    df_graph = pd.DataFrame({\n",
    "        'acct': list(all_accts),\n",
    "        'graph_degree': [total_degree[a] for a in all_accts],\n",
    "        'graph_bi_degree': [bi_degree[a] for a in all_accts],\n",
    "        'graph_reciprocity': [reci_ratio[a] for a in all_accts],\n",
    "    })\n",
    "\n",
    "    # Merge all\n",
    "    feat = pd.merge(left, right, on='acct', how='outer')\n",
    "    feat = feat.merge(df_graph, on='acct', how='left')\n",
    "\n",
    "    # Totals\n",
    "    feat['total_amt_sum'] = feat['send_amt_sum'].fillna(0) + feat['recv_amt_sum'].fillna(0)\n",
    "    feat['net_out_amt'] = feat['send_amt_sum'].fillna(0) - feat['recv_amt_sum'].fillna(0)\n",
    "    feat['total_cnt'] = feat['send_cnt'].fillna(0) + feat['recv_cnt'].fillna(0)\n",
    "\n",
    "    # Ratios\n",
    "    feat['send_avg_amt'] = _safe_div(feat['send_amt_sum'].fillna(0), feat['send_cnt'].replace(0, np.nan))\n",
    "    feat['recv_avg_amt'] = _safe_div(feat['recv_amt_sum'].fillna(0), feat['recv_cnt'].replace(0, np.nan))\n",
    "    feat['send_unique_rate'] = _safe_div(feat['send_unique_ctp'].fillna(0), feat['send_cnt'].replace(0, np.nan))\n",
    "    feat['recv_unique_rate'] = _safe_div(feat['recv_unique_ctp'].fillna(0), feat['recv_cnt'].replace(0, np.nan))\n",
    "\n",
    "    # Account type resolution\n",
    "    df_from_type = df[['from_acct', 'from_acct_type']].drop_duplicates().rename(columns={'from_acct': 'acct', 'from_acct_type': 'is_esun_from'})\n",
    "    df_to_type = df[['to_acct', 'to_acct_type']].drop_duplicates().rename(columns={'to_acct': 'acct', 'to_acct_type': 'is_esun_to'})\n",
    "    feat = feat.merge(df_from_type, on='acct', how='left').merge(df_to_type, on='acct', how='left')\n",
    "    feat['is_esun'] = feat[['is_esun_from', 'is_esun_to']].max(axis=1).fillna(0)\n",
    "\n",
    "    feat = feat.fillna(0)\n",
    "    print(f\"[OK] Feature engineering completed. Accounts: {len(feat)}; Features: {feat.shape[1]-1}\")\n",
    "    return feat\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 4. Train / Test Split\n",
    "# ======================================================================\n",
    "def make_splits(feat_df: pd.DataFrame, df_alert: pd.DataFrame, df_test: pd.DataFrame):\n",
    "    feat = feat_df.copy()\n",
    "\n",
    "    alert_set = set(df_alert['acct'].astype(str))\n",
    "    feat['label'] = feat['acct'].astype(str).isin(alert_set).astype(int)\n",
    "\n",
    "    test_set = set(df_test['acct'].astype(str))\n",
    "\n",
    "    # Train: exclude test accounts; Esun only (match test distribution)\n",
    "    train_df = feat[(~feat['acct'].astype(str).isin(test_set)) & (feat['is_esun'] == 1)].copy()\n",
    "    X = train_df.drop(columns=['label'])\n",
    "    y = train_df['label'].values\n",
    "\n",
    "    # Test: join features for the listed accounts; keep all columns\n",
    "    test_feat = feat[feat['acct'].astype(str).isin(test_set)].copy()\n",
    "    X_test = df_test[['acct']].merge(\n",
    "        test_feat.drop(columns=['label'], errors='ignore'),\n",
    "        on='acct', how='left'\n",
    "    ).fillna(0)\n",
    "\n",
    "    print(f\"[OK] Split -> Train accounts: {len(X)} (pos={sum(y)}, neg={len(y)-sum(y)}); Test accounts: {len(X_test)}\")\n",
    "    return X, y, X_test\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 5. Modeling (LightGBM + Stratified K-Fold)\n",
    "# ======================================================================\n",
    "def fit_model_kfold(X: pd.DataFrame, y: np.ndarray, random_state: int = 42, n_splits: int = 5):\n",
    "    \"\"\"\n",
    "    Train LightGBM using Stratified K-Fold; tune decision threshold on OOF predictions.\n",
    "    Returns: models, safe feature names, best threshold, rename_map\n",
    "    \"\"\"\n",
    "    # Exclude identifiers/control flags from model features\n",
    "    drop_cols = ['acct', 'is_esun', 'is_esun_from', 'is_esun_to']\n",
    "    feat_cols = [c for c in X.columns if c not in drop_cols]\n",
    "    X_features = X[feat_cols].copy()\n",
    "\n",
    "    # Sanitize feature names for LightGBM\n",
    "    def sanitize(name: str) -> str:\n",
    "        return (name.replace('[', '_')\n",
    "                    .replace(']', '_')\n",
    "                    .replace('<', '_')\n",
    "                    .replace('>', '_')\n",
    "                    .replace(':', '_')\n",
    "                    .replace('=', '_'))\n",
    "    safe_feat_cols = [sanitize(c) for c in feat_cols]\n",
    "    rename_map = dict(zip(feat_cols, safe_feat_cols))\n",
    "    X_safe = X_features.rename(columns=rename_map)\n",
    "\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': 6,\n",
    "        'seed': random_state,\n",
    "        'n_jobs': -1,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'subsample': 0.7,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'is_unbalance': True,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    oof_preds = np.zeros(len(X_safe))\n",
    "    models: List[lgb.LGBMClassifier] = []\n",
    "\n",
    "    print(\"--- Start LightGBM Stratified K-Fold Training ---\")\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(X_safe, y)):\n",
    "        X_tr, X_va = X_safe.iloc[tr_idx], X_safe.iloc[va_idx]\n",
    "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric='auc',\n",
    "            callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "        )\n",
    "\n",
    "        oof_preds[va_idx] = clf.predict_proba(X_va)[:, 1]\n",
    "        models.append(clf)\n",
    "        print(f\"Fold {fold+1} finished. AUC: {roc_auc_score(y_va, oof_preds[va_idx]):.4f}\")\n",
    "\n",
    "    # Threshold tuning on OOF\n",
    "    prec, rec, thr = precision_recall_curve(y, oof_preds)\n",
    "    f1s = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    best_idx = np.nanargmax(f1s)\n",
    "    best_thr = thr[best_idx] if best_idx < len(thr) else 0.5\n",
    "\n",
    "    y_oof_pred = (oof_preds >= best_thr).astype(int)\n",
    "    f1_oof = f1_score(y, y_oof_pred)\n",
    "\n",
    "    print(f\"\\n[OOF] Total AUC={roc_auc_score(y, oof_preds):.4f} üéØ F1@best={f1_oof:.4f} üß™ Threshold={best_thr:.4f}\")\n",
    "    print(\"[OOF] Classification report:\\n\", classification_report(y, y_oof_pred, digits=4))\n",
    "\n",
    "    return models, safe_feat_cols, best_thr, rename_map\n",
    "\n",
    "\n",
    "def predict_test_kfold(trained_models: List[lgb.LGBMClassifier],\n",
    "                       feat_cols: List[str],\n",
    "                       threshold: float,\n",
    "                       X_test: pd.DataFrame,\n",
    "                       rename_map: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Predict test probabilities by averaging over K models.\n",
    "    Uses the same rename_map as training to align feature names.\n",
    "    \"\"\"\n",
    "    # Apply same sanitization to X_test columns\n",
    "    X_test_safe = X_test.rename(columns=rename_map).copy()\n",
    "\n",
    "    # Ensure all expected columns exist\n",
    "    for c in feat_cols:\n",
    "        if c not in X_test_safe.columns:\n",
    "            X_test_safe[c] = 0\n",
    "\n",
    "    # Keep only the feature columns for prediction\n",
    "    proba_sum = np.zeros(len(X_test_safe))\n",
    "    for model in trained_models:\n",
    "        proba_sum += model.predict_proba(X_test_safe[feat_cols])[:, 1]\n",
    "\n",
    "    avg_proba = proba_sum / len(trained_models)\n",
    "    y_pred = (avg_proba >= threshold).astype(int)\n",
    "    return y_pred, avg_proba\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 6. Output\n",
    "# ======================================================================\n",
    "def save_submission(path: str, df_test: pd.DataFrame, X_test: pd.DataFrame, y_pred: np.ndarray):\n",
    "    df_pred = pd.DataFrame({'acct': X_test['acct'].values, 'label': y_pred.astype(int)})\n",
    "    out = df_test[['acct']].merge(df_pred, on='acct', how='left').fillna(0)\n",
    "    out.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK] Saved submission to: {path}\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 7. Main\n",
    "# ======================================================================\n",
    "def main():\n",
    "    dir_path = \"data\"  # change to your folder\n",
    "\n",
    "    try:\n",
    "        df_txn, df_alert, df_test = load_csvs(dir_path)\n",
    "        feat_df = engineer_features_enhanced(df_txn, df_alert)\n",
    "        X, y, X_test = make_splits(feat_df, df_alert, df_test)\n",
    "\n",
    "        models, feat_cols, thr, rename_map = fit_model_kfold(X, y, random_state=42, n_splits=5)\n",
    "        y_pred, _ = predict_test_kfold(models, feat_cols, thr, X_test, rename_map)\n",
    "\n",
    "        save_submission(\"enhanced_result.csv\", df_test, X_test, y_pred)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"[ERROR] File not found: {e}. Ensure CSVs exist in '{dir_path}' with correct names.\")\n",
    "    except ImportError:\n",
    "        print(\"\\nüö® ERROR: lightgbm not installed. Please: pip install lightgbm\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
