{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1221f946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded datasets.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 601\u001b[39m\n\u001b[32m    597\u001b[39m     save_feature_importance(models, feat_cols_safe, \u001b[33m\"\u001b[39m\u001b[33mfeature_importance.csv\u001b[39m\u001b[33m\"\u001b[39m, topn=cfg[\u001b[33m\"\u001b[39m\u001b[33msave_feature_importance_top\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 572\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    569\u001b[39m df_txn, df_alert, df_test = load_csvs(data_dir)\n\u001b[32m    571\u001b[39m \u001b[38;5;66;03m# 2) Feature engineering\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m feat_df = \u001b[43mengineer_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_txn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;66;03m# 3) Split\u001b[39;00m\n\u001b[32m    575\u001b[39m X, y, X_test = make_splits(feat_df, df_alert, df_test)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 227\u001b[39m, in \u001b[36mengineer_features\u001b[39m\u001b[34m(df_txn)\u001b[39m\n\u001b[32m    224\u001b[39m send_channel_props = []\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m top_channels:\n\u001b[32m    226\u001b[39m     send_channel_props.append(\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m         \u001b[43mg_from\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mch\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchannel_type_norm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.rename(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msend_ch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_prop\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    228\u001b[39m     )\n\u001b[32m    230\u001b[39m \u001b[38;5;66;03m# Currency mix\u001b[39;00m\n\u001b[32m    231\u001b[39m send_curr_nunique = g_from[\u001b[33m'\u001b[39m\u001b[33mcurrency_type_norm\u001b[39m\u001b[33m'\u001b[39m].nunique().rename(\u001b[33m'\u001b[39m\u001b[33msend_curr_nunique\u001b[39m\u001b[33m'\u001b[39m).astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\DM2025\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1826\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1824\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1825\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1826\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1827\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1828\u001b[39m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj, Series)\n\u001b[32m   1829\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1830\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.shape != \u001b[38;5;28mself\u001b[39m._obj_with_exclusions.shape\n\u001b[32m   1831\u001b[39m         ):\n\u001b[32m   1832\u001b[39m             warnings.warn(\n\u001b[32m   1833\u001b[39m                 message=_apply_groupings_depr.format(\n\u001b[32m   1834\u001b[39m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1837\u001b[39m                 stacklevel=find_stack_level(),\n\u001b[32m   1838\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\DM2025\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1887\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1852\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1853\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_python_apply_general\u001b[39m(\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1859\u001b[39m     is_agg: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1860\u001b[39m ) -> NDFrameT:\n\u001b[32m   1861\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1862\u001b[39m \u001b[33;03m    Apply function f in python space\u001b[39;00m\n\u001b[32m   1863\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1885\u001b[39m \u001b[33;03m        data after applying f\u001b[39;00m\n\u001b[32m   1886\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1887\u001b[39m     values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1888\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1889\u001b[39m         not_indexed_same = mutated\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\DM2025\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:928\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[32m    927\u001b[39m group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m res = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    930\u001b[39m     mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 227\u001b[39m, in \u001b[36mengineer_features.<locals>.<lambda>\u001b[39m\u001b[34m(s, ch)\u001b[39m\n\u001b[32m    224\u001b[39m send_channel_props = []\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m top_channels:\n\u001b[32m    226\u001b[39m     send_channel_props.append(\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m         g_from.apply(\u001b[38;5;28;01mlambda\u001b[39;00m s, ch=ch: (\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchannel_type_norm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mch\u001b[49m).mean()).rename(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msend_ch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_prop\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    228\u001b[39m     )\n\u001b[32m    230\u001b[39m \u001b[38;5;66;03m# Currency mix\u001b[39;00m\n\u001b[32m    231\u001b[39m send_curr_nunique = g_from[\u001b[33m'\u001b[39m\u001b[33mcurrency_type_norm\u001b[39m\u001b[33m'\u001b[39m].nunique().rename(\u001b[33m'\u001b[39m\u001b[33msend_curr_nunique\u001b[39m\u001b[33m'\u001b[39m).astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\DM2025\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\DM2025\\Lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[39m, in \u001b[36mOpsMixin.__eq__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__eq__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\DM2025\\Lib\\site-packages\\pandas\\core\\series.py:6130\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_cmp_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[32m-> \u001b[39m\u001b[32m6130\u001b[39m     res_name = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_op_result_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Series) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._indexed_same(other):\n\u001b[32m   6133\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan only compare identically-labeled Series objects\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\DM2025\\Lib\\site-packages\\pandas\\core\\ops\\common.py:96\u001b[39m, in \u001b[36mget_op_result_name\u001b[39m\u001b[34m(left, right)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_op_result_name\u001b[39m(left, right):\n\u001b[32m     82\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    Find the appropriate name to pin to an operation result.  This result\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m    should always be either an Index or a Series.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m \u001b[33;03m        Usually a string\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, (ABCSeries, ABCIndex)):\n\u001b[32m     97\u001b[39m         name = _maybe_match_name(left, right)\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\DM2025\\Lib\\site-packages\\pandas\\core\\dtypes\\generic.py:44\u001b[39m, in \u001b[36mcreate_pandas_abc_type.<locals>._instancecheck\u001b[39m\u001b[34m(cls, inst)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_instancecheck\u001b[39m(\u001b[38;5;28mcls\u001b[39m, inst) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inst, \u001b[38;5;28mtype\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\DM2025\\Lib\\site-packages\\pandas\\core\\dtypes\\generic.py:37\u001b[39m, in \u001b[36mcreate_pandas_abc_type.<locals>._check\u001b[39m\u001b[34m(inst)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_pandas_abc_type\u001b[39m(name, attr, comp):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check\u001b[39m(inst) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(inst, attr, \u001b[33m\"\u001b[39m\u001b[33m_typ\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m comp\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# https://github.com/python/mypy/issues/1006\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# error: 'classmethod' used with a non-method\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TransactionAlertPro_SOTA.py\n",
    "Strong Baseline for 2025 Esun AI Challenge — big F1 lift focus\n",
    "\n",
    "Key upgrades:\n",
    "1) Richer features: amount buckets (q25/q50/q75), channel entropy, burstiness, daily stats, graph reciprocity/degree\n",
    "2) Robust parsing + dtype downsizing (float32) for speed/memory\n",
    "3) LightGBM (GPU if available) + OOF F1 threshold tuning; optional Top-K / target rate thresholding\n",
    "4) Safe column sanitization + strict train/test parity; outputs feature importance for analysis\n",
    "\n",
    "Author: ChatGPT\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_recall_curve, average_precision_score,\n",
    "    roc_auc_score, classification_report\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Try LightGBM (GPU-friendly); fallback message if missing\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except Exception:\n",
    "    HAS_LGB = False\n",
    "    raise ImportError(\"lightgbm not installed. Please `conda install -c conda-forge lightgbm` (GPU build preferred).\")\n",
    "\n",
    "# ==========================\n",
    "# Config\n",
    "# ==========================\n",
    "CONFIG = {\n",
    "    \"data_dir\": \"data\",\n",
    "    \"cv_folds\": 5,\n",
    "    \"random_state\": 42,\n",
    "\n",
    "    # Threshold strategy: \"f1\" (OOF F1-best), \"topk\" (保留固定 K 名單), \"rate\" (保留固定比例)\n",
    "    \"threshold_mode\": \"f1\",\n",
    "    \"topk\": 400,                  # if mode == \"topk\"\n",
    "    \"target_positive_rate\": None, # e.g., 0.08 if mode == \"rate\"\n",
    "\n",
    "    # LightGBM settings (auto GPU if possible)\n",
    "    \"use_gpu_if_available\": True,\n",
    "    \"n_estimators\": 5000,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 63,\n",
    "    \"max_depth\": -1,\n",
    "    \"early_stopping_rounds\": 200,\n",
    "\n",
    "    # Safety: cap feature importance rows\n",
    "    \"save_feature_importance_top\": 200\n",
    "}\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Utils\n",
    "# ==========================\n",
    "def _to_numeric(s):\n",
    "    try:\n",
    "        return pd.to_numeric(s, errors=\"coerce\")\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _to_datetime(s):\n",
    "    try:\n",
    "        return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "def _ensure_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _to_hour(v):\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    s = str(v).strip()\n",
    "    s2 = ''.join(ch for ch in s if (ch.isdigit() or ch == ':'))\n",
    "    if ':' in s2:\n",
    "        try:\n",
    "            hh = int(s2.split(':')[0])\n",
    "            return hh if 0 <= hh <= 23 else np.nan\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    try:\n",
    "        s2 = s2.zfill(6)\n",
    "        hh = int(s2[:2])\n",
    "        return hh if 0 <= hh <= 23 else np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    \"\"\"Return aligned Series when inputs are Series; else scalar/array; zeros where invalid.\"\"\"\n",
    "    if isinstance(a, (pd.Series, pd.DataFrame)) or isinstance(b, (pd.Series, pd.DataFrame)):\n",
    "        mask = pd.notna(b) & (b != 0)\n",
    "        arr = np.where(mask, a / b, 0.0)\n",
    "        if isinstance(a, pd.Series):\n",
    "            return pd.Series(arr, index=a.index)\n",
    "        elif isinstance(b, pd.Series):\n",
    "            return pd.Series(arr, index=b.index)\n",
    "        else:\n",
    "            return arr\n",
    "    else:\n",
    "        return (a / b) if (b is not None and b != 0) else 0.0\n",
    "\n",
    "def _map_is_esun(v):\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    s = str(v).strip()\n",
    "    return 1 if s in {\"1\", \"01\", \"esun\", \"ESUN\", \"玉山\"} else 0\n",
    "\n",
    "def _entropy(probs: np.ndarray) -> float:\n",
    "    p = probs[probs > 0]\n",
    "    if p.size == 0:\n",
    "        return 0.0\n",
    "    return float(-(p * np.log(p)).sum())\n",
    "\n",
    "def _sanitize_col(name: str) -> str:\n",
    "    return (name.replace('[', '_')\n",
    "                .replace(']', '_')\n",
    "                .replace('<', '_')\n",
    "                .replace('>', '_')\n",
    "                .replace(':', '_')\n",
    "                .replace('=', '_')\n",
    "                .replace(' ', '_'))\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# I/O\n",
    "# ==========================\n",
    "def load_csvs(dir_path: str):\n",
    "    df_txn = pd.read_csv(os.path.join(dir_path, 'acct_transaction.csv'))\n",
    "    df_alert = pd.read_csv(os.path.join(dir_path, 'acct_alert.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(dir_path, 'acct_predict.csv'))\n",
    "    print(\"[OK] Loaded datasets.\")\n",
    "    return df_txn, df_alert, df_test\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Feature Engineering\n",
    "# ==========================\n",
    "def engineer_features(df_txn: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a strong set of account-level features from row-level transactions.\n",
    "    Includes: amount stats + buckets, channel entropy, burstiness, daily activity,\n",
    "              hour/night patterns, currency mix, and graph-like features.\n",
    "    \"\"\"\n",
    "    df = df_txn.copy()\n",
    "\n",
    "    # ---- Typing ----\n",
    "    if 'txn_amt' not in df.columns:\n",
    "        raise ValueError(\"Missing txn_amt.\")\n",
    "    df['txn_amt'] = df['txn_amt'].apply(_to_numeric).astype('float32')\n",
    "\n",
    "    df['txn_date_dt'] = _to_datetime(df['txn_date']) if 'txn_date' in df.columns else pd.NaT\n",
    "    df['hour'] = df['txn_time'].apply(_to_hour) if 'txn_time' in df.columns else np.nan\n",
    "    df['hour'] = df['hour'].astype('float32')\n",
    "\n",
    "    for col in ['from_acct_type', 'to_acct_type']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(_map_is_esun).astype('float32')\n",
    "        else:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # binary self-transfer\n",
    "    if 'is_self_txn' in df.columns:\n",
    "        df['is_self_txn_f'] = df['is_self_txn'].map(lambda x: 1 if str(x).strip().upper() == 'Y' else (0 if str(x).strip().upper() == 'N' else np.nan))\n",
    "    else:\n",
    "        df['is_self_txn_f'] = np.nan\n",
    "    df['is_self_txn_f'] = df['is_self_txn_f'].astype('float32')\n",
    "\n",
    "    # Normalize categories\n",
    "    df['channel_type_norm'] = df['channel_type'].astype(str).str.strip().fillna('UNK') if 'channel_type' in df.columns else 'UNK'\n",
    "    df['currency_type_norm'] = df['currency_type'].astype(str).str.strip().fillna('UNK') if 'currency_type' in df.columns else 'UNK'\n",
    "\n",
    "    if not set(['from_acct', 'to_acct']).issubset(df.columns):\n",
    "        raise ValueError(\"from_acct / to_acct required.\")\n",
    "\n",
    "    # ---- Global amount buckets (q25/q50/q75) ----\n",
    "    amt = df['txn_amt'].dropna().values\n",
    "    if amt.size > 0:\n",
    "        q25, q50, q75 = np.quantile(amt, [0.25, 0.50, 0.75])\n",
    "    else:\n",
    "        q25 = q50 = q75 = 0.0\n",
    "\n",
    "    def _bucket(a):\n",
    "        if pd.isna(a): return 'UNK'\n",
    "        if a <= q25: return 'S'     # small\n",
    "        elif a <= q50: return 'SM'  # small-mid\n",
    "        elif a <= q75: return 'ML'  # mid-large\n",
    "        else: return 'L'            # large\n",
    "    df['amt_bucket'] = df['txn_amt'].apply(_bucket)\n",
    "\n",
    "    # ---- Sender aggregates ----\n",
    "    g_from = df.groupby('from_acct', observed=True)\n",
    "\n",
    "    send_cnt = g_from.size().rename('send_cnt').astype('float32')\n",
    "    send_amt_agg = g_from['txn_amt'].agg(['sum', 'mean', 'std', 'max', 'min', 'median']).rename(\n",
    "        columns=lambda c: f\"send_amt_{c}\"\n",
    "    ).astype('float32')\n",
    "\n",
    "    send_unique_ctp = g_from['to_acct'].nunique().rename('send_unique_ctp').astype('float32')\n",
    "    send_hour_mean = g_from['hour'].mean().rename('send_hour_mean').astype('float32')\n",
    "    send_hour_std = g_from['hour'].std(ddof=0).fillna(0).rename('send_hour_std').astype('float32')\n",
    "    send_night_cnt = g_from['hour'].apply(lambda s: ((s>=22) | (s<=6)).sum()).rename('send_night_cnt').astype('float32')\n",
    "    send_self_cnt = g_from['is_self_txn_f'].sum(min_count=1).fillna(0).rename('send_self_cnt').astype('float32')\n",
    "    send_self_ratio = _safe_div(send_self_cnt, send_cnt.replace(0, np.nan)).fillna(0).rename('send_self_ratio').astype('float32')\n",
    "    send_to_esun_ratio = g_from['to_acct_type'].mean().rename('send_to_esun_ratio').astype('float32')\n",
    "\n",
    "    # Channel mix (top 8)\n",
    "    top_channels = df['channel_type_norm'].value_counts().head(8).index.tolist()\n",
    "    send_channel_props = []\n",
    "    for ch in top_channels:\n",
    "        send_channel_props.append(\n",
    "            g_from.apply(lambda s, ch=ch: (s['channel_type_norm'] == ch).mean()).rename(f\"send_ch_{ch}_prop\").astype('float32')\n",
    "        )\n",
    "\n",
    "    # Currency mix\n",
    "    send_curr_nunique = g_from['currency_type_norm'].nunique().rename('send_curr_nunique').astype('float32')\n",
    "    send_curr_twd_ratio = g_from.apply(lambda s: (s['currency_type_norm']=='TWD').mean()).rename('send_curr_twd_ratio').astype('float32')\n",
    "\n",
    "    # Daily activity (burstiness)\n",
    "    if 'txn_date_dt' in df.columns:\n",
    "        df_day_from = df.dropna(subset=['txn_date_dt']).groupby(['from_acct', df['txn_date_dt'].dt.date]).agg(\n",
    "            day_amt_sum=('txn_amt', 'sum'),\n",
    "            day_cnt=('txn_amt', 'count')\n",
    "        ).reset_index().rename(columns={'txn_date_dt': 'date'})\n",
    "        gdf = df_day_from.groupby('from_acct', observed=True)\n",
    "        send_active_days = gdf['date'].nunique().rename('send_active_days').astype('float32')\n",
    "        send_day_cnt_mean = gdf['day_cnt'].mean().rename('send_day_cnt_mean').astype('float32')\n",
    "        send_day_cnt_std  = gdf['day_cnt'].std(ddof=0).fillna(0).rename('send_day_cnt_std').astype('float32')\n",
    "        send_burstiness   = _safe_div(send_day_cnt_std, send_day_cnt_mean.replace(0, np.nan)).fillna(0).rename('send_burstiness').astype('float32')\n",
    "        # recency gap\n",
    "        ref_date = df['txn_date_dt'].max()\n",
    "        send_last_gap_days = g_from['txn_date_dt'].max().apply(lambda t: (ref_date - t).days if pd.notna(t) else np.nan)\\\n",
    "                              .fillna(9999).rename('send_last_gap_days').astype('float32')\n",
    "    else:\n",
    "        send_active_days = send_day_cnt_mean = send_day_cnt_std = send_burstiness = send_last_gap_days = pd.Series(dtype='float32')\n",
    "\n",
    "    # Amount buckets proportion\n",
    "    send_bucket_props = []\n",
    "    for b in ['S','SM','ML','L']:\n",
    "        send_bucket_props.append(\n",
    "            g_from.apply(lambda s, b=b: (s['amt_bucket']==b).mean()).rename(f\"send_amt_{b}_prop\").astype('float32')\n",
    "        )\n",
    "\n",
    "    left = pd.concat([\n",
    "        send_cnt, send_amt_agg, send_unique_ctp,\n",
    "        send_hour_mean, send_hour_std, send_night_cnt,\n",
    "        send_self_cnt, send_self_ratio, send_to_esun_ratio,\n",
    "        send_curr_nunique, send_curr_twd_ratio,\n",
    "        send_active_days, send_day_cnt_mean, send_day_cnt_std, send_burstiness, send_last_gap_days\n",
    "    ] + send_channel_props + send_bucket_props, axis=1).reset_index().rename(columns={'from_acct':'acct'})\n",
    "\n",
    "    # ---- Receiver aggregates ----\n",
    "    g_to = df.groupby('to_acct', observed=True)\n",
    "\n",
    "    recv_cnt = g_to.size().rename('recv_cnt').astype('float32')\n",
    "    recv_amt_agg = g_to['txn_amt'].agg(['sum', 'mean', 'std', 'max', 'min', 'median']).rename(\n",
    "        columns=lambda c: f\"recv_amt_{c}\"\n",
    "    ).astype('float32')\n",
    "\n",
    "    recv_unique_ctp = g_to['from_acct'].nunique().rename('recv_unique_ctp').astype('float32')\n",
    "    recv_hour_mean = g_to['hour'].mean().rename('recv_hour_mean').astype('float32')\n",
    "    recv_hour_std = g_to['hour'].std(ddof=0).fillna(0).rename('recv_hour_std').astype('float32')\n",
    "    recv_night_cnt = g_to['hour'].apply(lambda s: ((s>=22) | (s<=6)).sum()).rename('recv_night_cnt').astype('float32')\n",
    "    recv_self_cnt = g_to['is_self_txn_f'].sum(min_count=1).fillna(0).rename('recv_self_cnt').astype('float32')\n",
    "    recv_self_ratio = _safe_div(recv_self_cnt, recv_cnt.replace(0, np.nan)).fillna(0).rename('recv_self_ratio').astype('float32')\n",
    "    recv_from_esun_ratio = g_to['from_acct_type'].mean().rename('recv_from_esun_ratio').astype('float32')\n",
    "\n",
    "    recv_channel_props = []\n",
    "    for ch in top_channels:\n",
    "        recv_channel_props.append(\n",
    "            g_to.apply(lambda s, ch=ch: (s['channel_type_norm'] == ch).mean()).rename(f\"recv_ch_{ch}_prop\").astype('float32')\n",
    "        )\n",
    "\n",
    "    recv_curr_nunique = g_to['currency_type_norm'].nunique().rename('recv_curr_nunique').astype('float32')\n",
    "    recv_curr_twd_ratio = g_to.apply(lambda s: (s['currency_type_norm']=='TWD').mean()).rename('recv_curr_twd_ratio').astype('float32')\n",
    "\n",
    "    # daily stats for receiver\n",
    "    if 'txn_date_dt' in df.columns:\n",
    "        df_day_to = df.dropna(subset=['txn_date_dt']).groupby(['to_acct', df['txn_date_dt'].dt.date]).agg(\n",
    "            day_amt_sum=('txn_amt', 'sum'),\n",
    "            day_cnt=('txn_amt', 'count')\n",
    "        ).reset_index().rename(columns={'txn_date_dt': 'date'})\n",
    "        gdt = df_day_to.groupby('to_acct', observed=True)\n",
    "        recv_active_days = gdt['date'].nunique().rename('recv_active_days').astype('float32')\n",
    "        recv_day_cnt_mean = gdt['day_cnt'].mean().rename('recv_day_cnt_mean').astype('float32')\n",
    "        recv_day_cnt_std  = gdt['day_cnt'].std(ddof=0).fillna(0).rename('recv_day_cnt_std').astype('float32')\n",
    "        recv_burstiness   = _safe_div(recv_day_cnt_std, recv_day_cnt_mean.replace(0, np.nan)).fillna(0).rename('recv_burstiness').astype('float32')\n",
    "        ref_date = df['txn_date_dt'].max()\n",
    "        recv_last_gap_days = g_to['txn_date_dt'].max().apply(lambda t: (ref_date - t).days if pd.notna(t) else np.nan)\\\n",
    "                              .fillna(9999).rename('recv_last_gap_days').astype('float32')\n",
    "    else:\n",
    "        recv_active_days = recv_day_cnt_mean = recv_day_cnt_std = recv_burstiness = recv_last_gap_days = pd.Series(dtype='float32')\n",
    "\n",
    "    recv_bucket_props = []\n",
    "    for b in ['S','SM','ML','L']:\n",
    "        recv_bucket_props.append(\n",
    "            g_to.apply(lambda s, b=b: (s['amt_bucket']==b).mean()).rename(f\"recv_amt_{b}_prop\").astype('float32')\n",
    "        )\n",
    "\n",
    "    right = pd.concat([\n",
    "        recv_cnt, recv_amt_agg, recv_unique_ctp,\n",
    "        recv_hour_mean, recv_hour_std, recv_night_cnt,\n",
    "        recv_self_cnt, recv_self_ratio, recv_from_esun_ratio,\n",
    "        recv_curr_nunique, recv_curr_twd_ratio,\n",
    "        recv_active_days, recv_day_cnt_mean, recv_day_cnt_std, recv_burstiness, recv_last_gap_days\n",
    "    ] + recv_channel_props + recv_bucket_props, axis=1).reset_index().rename(columns={'to_acct':'acct'})\n",
    "\n",
    "    # ---- Graph-like features ----\n",
    "    out_sets = df.groupby('from_acct')['to_acct'].apply(set)\n",
    "    in_sets = df.groupby('to_acct')['from_acct'].apply(set)\n",
    "    all_accts = set(out_sets.index).union(set(in_sets.index))\n",
    "\n",
    "    reci_ratio = {}\n",
    "    total_degree = {}\n",
    "    bi_degree = {}\n",
    "    for a in all_accts:\n",
    "        outs = out_sets.get(a, set())\n",
    "        ins  = in_sets.get(a, set())\n",
    "        deg = len(outs.union(ins))\n",
    "        bi  = len(outs.intersection(ins))\n",
    "        total_degree[a] = deg\n",
    "        bi_degree[a]    = bi\n",
    "        reci_ratio[a]   = _safe_div(bi, deg)\n",
    "\n",
    "    df_graph = pd.DataFrame({\n",
    "        'acct': list(all_accts),\n",
    "        'graph_degree': [total_degree[a] for a in all_accts],\n",
    "        'graph_bi_degree': [bi_degree[a] for a in all_accts],\n",
    "        'graph_reciprocity': [reci_ratio[a] for a in all_accts],\n",
    "    }).astype({'graph_degree':'float32','graph_bi_degree':'float32','graph_reciprocity':'float32'})\n",
    "\n",
    "    # ---- Merge sender/receiver/graph ----\n",
    "    feat = pd.merge(left, right, on='acct', how='outer')\n",
    "    feat = feat.merge(df_graph, on='acct', how='left')\n",
    "\n",
    "    # Totals & normalized ratios\n",
    "    feat['total_amt_sum'] = feat['send_amt_sum'].fillna(0) + feat['recv_amt_sum'].fillna(0)\n",
    "    feat['net_out_amt']   = feat['send_amt_sum'].fillna(0) - feat['recv_amt_sum'].fillna(0)\n",
    "    feat['total_cnt']     = feat['send_cnt'].fillna(0)     + feat['recv_cnt'].fillna(0)\n",
    "    feat['send_avg_amt']  = _safe_div(feat['send_amt_sum'].fillna(0), feat['send_cnt'].replace(0, np.nan)).fillna(0)\n",
    "    feat['recv_avg_amt']  = _safe_div(feat['recv_amt_sum'].fillna(0), feat['recv_cnt'].replace(0, np.nan)).fillna(0)\n",
    "    feat['send_unique_rate'] = _safe_div(feat['send_unique_ctp'].fillna(0), feat['send_cnt'].replace(0, np.nan)).fillna(0)\n",
    "    feat['recv_unique_rate'] = _safe_div(feat['recv_unique_ctp'].fillna(0), feat['recv_cnt'].replace(0, np.nan)).fillna(0)\n",
    "\n",
    "    # Channel entropy（sender/receiver）\n",
    "    # sender distribution\n",
    "    send_ch_dist = df.groupby(['from_acct','channel_type_norm']).size().groupby(level=0).apply(lambda s: s / s.sum())\\\n",
    "                      .unstack(fill_value=0)\n",
    "    send_ch_entropy = send_ch_dist.apply(lambda row: _entropy(row.values.astype(np.float64)), axis=1)\\\n",
    "                                  .rename('send_ch_entropy').reset_index().rename(columns={'from_acct':'acct'})\n",
    "    # receiver distribution\n",
    "    recv_ch_dist = df.groupby(['to_acct','channel_type_norm']).size().groupby(level=0).apply(lambda s: s / s.sum())\\\n",
    "                      .unstack(fill_value=0)\n",
    "    recv_ch_entropy = recv_ch_dist.apply(lambda row: _entropy(row.values.astype(np.float64)), axis=1)\\\n",
    "                                  .rename('recv_ch_entropy').reset_index().rename(columns={'to_acct':'acct'})\n",
    "\n",
    "    feat = feat.merge(send_ch_entropy, on='acct', how='left').merge(recv_ch_entropy, on='acct', how='left')\n",
    "\n",
    "    # Account type resolution (Esun flag)\n",
    "    df_from_type = df[['from_acct', 'from_acct_type']].drop_duplicates().rename(columns={'from_acct':'acct','from_acct_type':'is_esun_from'})\n",
    "    df_to_type   = df[['to_acct', 'to_acct_type']].drop_duplicates().rename(columns={'to_acct':'acct','to_acct_type':'is_esun_to'})\n",
    "    feat = feat.merge(df_from_type, on='acct', how='left').merge(df_to_type, on='acct', how='left')\n",
    "    feat['is_esun'] = feat[['is_esun_from','is_esun_to']].max(axis=1).fillna(0)\n",
    "\n",
    "    # Dtype downsizing\n",
    "    for c in feat.columns:\n",
    "        if c != 'acct' and pd.api.types.is_float_dtype(feat[c]):\n",
    "            feat[c] = feat[c].astype('float32')\n",
    "\n",
    "    feat = feat.fillna(0)\n",
    "    print(f\"[OK] Feature engineering completed. Accounts={len(feat)} Features={feat.shape[1]-1}\")\n",
    "    return feat\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Split & Labels\n",
    "# ==========================\n",
    "def make_splits(feat_df: pd.DataFrame, df_alert: pd.DataFrame, df_test: pd.DataFrame):\n",
    "    feat = feat_df.copy()\n",
    "    alert_set = set(df_alert['acct'].astype(str))\n",
    "    feat['label'] = feat['acct'].astype(str).isin(alert_set).astype(int)\n",
    "\n",
    "    test_set = set(df_test['acct'].astype(str))\n",
    "\n",
    "    train_df = feat[(~feat['acct'].astype(str).isin(test_set)) & (feat['is_esun'] == 1)].copy()\n",
    "    X = train_df.drop(columns=['label'])\n",
    "    y = train_df['label'].values\n",
    "\n",
    "    # 保證 test 欄位與 train 對齊\n",
    "    test_feat = feat[feat['acct'].astype(str).isin(test_set)].copy()\n",
    "    X_test = df_test[['acct']].merge(test_feat.drop(columns=['label'], errors='ignore'), on='acct', how='left').fillna(0)\n",
    "\n",
    "    print(f\"[OK] Split -> Train={len(X)} (pos={sum(y)}, neg={len(y)-sum(y)}); Test={len(X_test)}\")\n",
    "    return X, y, X_test\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Modeling (LightGBM + OOF threshold)\n",
    "# ==========================\n",
    "def fit_lgbm_kfold(X: pd.DataFrame, y: np.ndarray, cfg: Dict):\n",
    "    drop_cols = ['acct', 'is_esun', 'is_esun_from', 'is_esun_to']\n",
    "    feat_cols = [c for c in X.columns if c not in drop_cols]\n",
    "\n",
    "    # Sanitize names for LGB\n",
    "    rename_map = {c: _sanitize_col(c) for c in feat_cols}\n",
    "    Xs = X[feat_cols].rename(columns=rename_map).copy()\n",
    "    safe_cols = list(Xs.columns)\n",
    "\n",
    "    # class weight\n",
    "    pos = y.sum()\n",
    "    neg = len(y) - pos\n",
    "    scale_pos_weight = max((neg / max(pos, 1)), 1.0)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': cfg[\"n_estimators\"],\n",
    "        'learning_rate': cfg[\"learning_rate\"],\n",
    "        'num_leaves': cfg[\"num_leaves\"],\n",
    "        'max_depth': cfg[\"max_depth\"],\n",
    "        'seed': cfg[\"random_state\"],\n",
    "        'n_jobs': -1,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'subsample': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'is_unbalance': True,\n",
    "        'verbose': -1,\n",
    "        'scale_pos_weight': scale_pos_weight\n",
    "    }\n",
    "\n",
    "    # GPU if available/required\n",
    "    if cfg[\"use_gpu_if_available\"]:\n",
    "        params.update({\n",
    "            'device': 'gpu',\n",
    "            'device_type': 'gpu',   # 某些版本用 device_type\n",
    "            'gpu_use_dp': False\n",
    "        })\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=cfg[\"cv_folds\"], shuffle=True, random_state=cfg[\"random_state\"])\n",
    "    oof = np.zeros(len(Xs), dtype=np.float64)\n",
    "    models: List[lgb.LGBMClassifier] = []\n",
    "\n",
    "    print(\"--- LightGBM Stratified K-Fold ---\")\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(Xs, y), 1):\n",
    "        Xt, Xv = Xs.iloc[tr_idx], Xs.iloc[va_idx]\n",
    "        yt, yv = y[tr_idx], y[va_idx]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**params)\n",
    "        clf.fit(\n",
    "            Xt, yt,\n",
    "            eval_set=[(Xv, yv)],\n",
    "            eval_metric='auc',\n",
    "            callbacks=[lgb.early_stopping(CONFIG[\"early_stopping_rounds\"], verbose=False)]\n",
    "        )\n",
    "        oof[va_idx] = clf.predict_proba(Xv)[:, 1]\n",
    "        models.append(clf)\n",
    "        print(f\"Fold {fold}: AUC={roc_auc_score(yv, oof[va_idx]):.4f}\")\n",
    "\n",
    "    # OOF metrics\n",
    "    ap = average_precision_score(y, oof)\n",
    "    auc = roc_auc_score(y, oof)\n",
    "    prec, rec, thr = precision_recall_curve(y, oof)\n",
    "    f1s = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    best_idx = np.nanargmax(f1s)\n",
    "    f1_best = f1s[best_idx] if best_idx < len(f1s) else 0.0\n",
    "    thr_f1 = thr[best_idx] if best_idx < len(thr) else 0.5\n",
    "\n",
    "    print(f\"[OOF] AP={ap:.4f} AUC={auc:.4f} F1@best={f1_best:.4f} thr={thr_f1:.4f}\")\n",
    "    y_oof = (oof >= thr_f1).astype(int)\n",
    "    print(\"[OOF] Classification report:\\n\", classification_report(y, y_oof, digits=4))\n",
    "    return models, safe_cols, rename_map, oof, thr_f1\n",
    "\n",
    "\n",
    "def choose_threshold(oof: np.ndarray, y: np.ndarray, cfg: Dict) -> float:\n",
    "    mode = cfg[\"threshold_mode\"]\n",
    "    if mode == \"f1\":\n",
    "        prec, rec, thr = precision_recall_curve(y, oof)\n",
    "        f1s = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "        best_idx = np.nanargmax(f1s)\n",
    "        return float(thr[best_idx] if best_idx < len(thr) else 0.5)\n",
    "    elif mode == \"topk\":\n",
    "        k = int(cfg[\"topk\"])\n",
    "        if k <= 0: return 0.5\n",
    "        # 閥值 = 第 k 名的分數（由高到低）\n",
    "        cut = np.partition(-oof, k-1)[k-1]\n",
    "        return float(-cut - 1e-12)\n",
    "    elif mode == \"rate\":\n",
    "        rate = float(cfg[\"target_positive_rate\"] or 0.1)\n",
    "        q = 1.0 - rate\n",
    "        return float(np.quantile(oof, q))\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "\n",
    "def predict_on_test(models: List[lgb.LGBMClassifier],\n",
    "                    feat_cols_safe: List[str],\n",
    "                    rename_map: Dict[str, str],\n",
    "                    X_test: pd.DataFrame,\n",
    "                    threshold: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # 應用同樣的欄名淨化\n",
    "    rev_map = rename_map  # 原->淨化\n",
    "    Xt = X_test[[c for c in X_test.columns if c != 'acct']].rename(columns=rev_map).copy()\n",
    "\n",
    "    # 確保所有訓練欄位存在\n",
    "    for c in feat_cols_safe:\n",
    "        if c not in Xt.columns:\n",
    "            Xt[c] = 0\n",
    "\n",
    "    # 只留訓練用欄位\n",
    "    Xt = Xt[feat_cols_safe]\n",
    "    proba = np.zeros(len(Xt), dtype=np.float64)\n",
    "    for m in models:\n",
    "        proba += m.predict_proba(Xt)[:, 1]\n",
    "    proba /= len(models)\n",
    "\n",
    "    y_pred = (proba >= threshold).astype(int)\n",
    "    return y_pred, proba\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Save artifacts\n",
    "# ==========================\n",
    "def save_submission(path: str, df_test: pd.DataFrame, X_test: pd.DataFrame, y_pred: np.ndarray):\n",
    "    out = pd.DataFrame({'acct': X_test['acct'].values, 'label': y_pred.astype(int)})\n",
    "    out = df_test[['acct']].merge(out, on='acct', how='left').fillna(0)\n",
    "    out.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK] Saved submission => {path}\")\n",
    "\n",
    "def save_feature_importance(models: List[lgb.LGBMClassifier],\n",
    "                            feat_cols_safe: List[str],\n",
    "                            path: str,\n",
    "                            topn: int = 200):\n",
    "    # 平均 gain importance\n",
    "    imps = np.zeros(len(feat_cols_safe))\n",
    "    for m in models:\n",
    "        imps += m.booster_.feature_importance(importance_type='gain')\n",
    "    imps /= max(len(models), 1)\n",
    "    df_imp = pd.DataFrame({'feature': feat_cols_safe, 'gain_importance': imps})\n",
    "    df_imp = df_imp.sort_values('gain_importance', ascending=False).head(topn)\n",
    "    df_imp.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK] Saved feature importance => {path}\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Main\n",
    "# ==========================\n",
    "def main():\n",
    "    cfg = CONFIG.copy()\n",
    "    data_dir = cfg[\"data_dir\"]\n",
    "\n",
    "    # 1) Load\n",
    "    df_txn, df_alert, df_test = load_csvs(data_dir)\n",
    "\n",
    "    # 2) Feature engineering\n",
    "    feat_df = engineer_features(df_txn)\n",
    "\n",
    "    # 3) Split\n",
    "    X, y, X_test = make_splits(feat_df, df_alert, df_test)\n",
    "\n",
    "    # 4) Train + OOF threshold\n",
    "    models, feat_cols_safe, rename_map, oof, thr_f1 = fit_lgbm_kfold(X, y, cfg)\n",
    "        # 印出目前使用的裝置\n",
    "    device_used = \"GPU\" if cfg[\"use_gpu_if_available\"] and any(\n",
    "        \"gpu\" in str(m.get_params().get(\"device\", \"\")).lower()\n",
    "        or \"gpu\" in str(m.get_params().get(\"device_type\", \"\")).lower()\n",
    "        for m in models\n",
    "    ) else \"CPU\"\n",
    "    print(f\"[INFO] LightGBM is running on: {device_used}\")\n",
    "\n",
    "\n",
    "    # choose strategy (F1 / TopK / Rate)\n",
    "    thr = choose_threshold(oof, y, cfg)\n",
    "    print(f\"[THR] Selected threshold ({cfg['threshold_mode']}): {thr:.6f}\")\n",
    "\n",
    "    # 5) Predict test\n",
    "    y_pred, proba = predict_on_test(models, feat_cols_safe, rename_map, X_test, thr)\n",
    "\n",
    "    # 6) Save\n",
    "    save_submission(\"enhanced_result.csv\", df_test, X_test, y_pred)\n",
    "    save_feature_importance(models, feat_cols_safe, \"feature_importance.csv\", topn=cfg[\"save_feature_importance_top\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
