{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0191aa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded datasets.\n",
      "[OK] Feature engineering completed. Accounts: 1800106; Features: 67\n",
      "[OK] Split -> Train accounts: 328988 (pos=1004, neg=327984); Test accounts: 4780\n",
      "[VAL] AP=0.5364  AUC=0.9603  F1@best=0.5826  thr=0.1500\n",
      "[VAL] Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9985    0.9992    0.9989     65597\n",
      "           1     0.6667    0.5174    0.5826       201\n",
      "\n",
      "    accuracy                         0.9977     65798\n",
      "   macro avg     0.8326    0.7583    0.7907     65798\n",
      "weighted avg     0.9975    0.9977    0.9976     65798\n",
      "\n",
      "[OK] Saved submission to: result.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TransactionAlertPro.py\n",
    "Enhanced Baseline for 2025 Esun AI Challenge (Binary Classification)\n",
    "\n",
    "Key upgrades over the sample:\n",
    "1) Rich account-level feature engineering (amount stats, behavior, graph-like features, channel/currency/self-txn mix, time recency & activity)\n",
    "2) Robust parsing for date/time and numeric values\n",
    "3) Class-imbalance handling and threshold tuning via validation set (F1 optimization)\n",
    "4) Strong but simple model (RandomForestClassifier) + reproducibility\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, average_precision_score, roc_auc_score, classification_report\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def _to_numeric(s):\n",
    "    \"\"\"Best-effort numeric conversion; returns float (NaN if fail).\"\"\"\n",
    "    try:\n",
    "        return pd.to_numeric(s, errors=\"coerce\")\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _to_datetime(s):\n",
    "    \"\"\"Best-effort datetime parsing.\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    \"\"\"\n",
    "    Safe division that works for scalars, numpy arrays, and pandas Series/DataFrames.\n",
    "    Returns 0.0 where b is NaN or 0.\n",
    "    \"\"\"\n",
    "    # 如果有任一是 pandas 物件（Series/DataFrame），用向量化邏輯\n",
    "    if isinstance(a, (pd.Series, pd.DataFrame)) or isinstance(b, (pd.Series, pd.DataFrame)):\n",
    "        # 將 b 轉為與 a 可廣播的陣列/序列來做遮罩\n",
    "        mask = pd.notna(b) & (b != 0)\n",
    "        # 使用 np.where 做逐元素安全除法\n",
    "        return np.where(mask, a / b, 0.0)\n",
    "    # 否則視為標量\n",
    "    else:\n",
    "        return (a / b) if (b is not None and b != 0) else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def _most_common(values: pd.Series, topk: int = 5) -> Dict:\n",
    "    \"\"\"Return counts of top-k most frequent elements (as dict).\"\"\"\n",
    "    cnt = values.value_counts()\n",
    "    top = cnt.head(topk)\n",
    "    return {f\"top_{i+1}_val\": top.index[i] if i < len(top) else np.nan for i in range(topk)}\n",
    "\n",
    "\n",
    "def _map_is_esun(v):\n",
    "    \"\"\"\n",
    "    Map acct_type to 1 (esun) / 0 (others).\n",
    "    Per spec: 01: 玉山；02: 他行（可能為字串'01','1'等）\n",
    "    \"\"\"\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    s = str(v).strip()\n",
    "    return 1 if s in {\"1\", \"01\", \"esun\", \"ESUN\", \"玉山\"} else 0\n",
    "\n",
    "\n",
    "def _ensure_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data Loading\n",
    "# -----------------------------\n",
    "def load_csvs(dir_path: str):\n",
    "    df_txn = pd.read_csv(os.path.join(dir_path, 'acct_transaction.csv'))\n",
    "    df_alert = pd.read_csv(os.path.join(dir_path, 'acct_alert.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(dir_path, 'acct_predict.csv'))\n",
    "    print(\"[OK] Loaded datasets.\")\n",
    "    return df_txn, df_alert, df_test\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Feature Engineering\n",
    "# -----------------------------\n",
    "def engineer_features(df_txn: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build robust account-level features using both sender and receiver roles.\n",
    "    \"\"\"\n",
    "    df = df_txn.copy()\n",
    "\n",
    "    # --- Robust typing ---\n",
    "    # amounts\n",
    "    if 'txn_amt' in df.columns:\n",
    "        df['txn_amt'] = df['txn_amt'].apply(_to_numeric)\n",
    "    else:\n",
    "        raise ValueError(\"Column 'txn_amt' not found in transaction data.\")\n",
    "\n",
    "    # date & time\n",
    "    if 'txn_date' in df.columns:\n",
    "        df['txn_date_dt'] = df['txn_date'].apply(_to_datetime)\n",
    "    else:\n",
    "        df['txn_date_dt'] = pd.NaT\n",
    "\n",
    "    if 'txn_time' in df.columns:\n",
    "        # try to parse as HHMMSS or HH:MM:SS\n",
    "        def _to_hour(v):\n",
    "            if pd.isna(v):\n",
    "                return np.nan\n",
    "            s = str(v).strip()\n",
    "            # remove non-digits except ':'\n",
    "            s2 = ''.join(ch for ch in s if (ch.isdigit() or ch == ':'))\n",
    "            if ':' in s2:\n",
    "                try:\n",
    "                    hh = int(s2.split(':')[0])\n",
    "                    return hh if 0 <= hh <= 23 else np.nan\n",
    "                except Exception:\n",
    "                    return np.nan\n",
    "            # assume integer HHMMSS\n",
    "            try:\n",
    "                s2 = s2.zfill(6)\n",
    "                hh = int(s2[:2])\n",
    "                return hh if 0 <= hh <= 23 else np.nan\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "\n",
    "        df['hour'] = df['txn_time'].apply(_to_hour)\n",
    "    else:\n",
    "        df['hour'] = np.nan\n",
    "\n",
    "    # flags / categories\n",
    "    for col in ['from_acct_type', 'to_acct_type']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(_map_is_esun)\n",
    "        else:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    if 'is_self_txn' in df.columns:\n",
    "        df['is_self_txn_f'] = df['is_self_txn'].map(lambda x: 1 if str(x).strip().upper() == 'Y' else (0 if str(x).strip().upper() == 'N' else np.nan))\n",
    "    else:\n",
    "        df['is_self_txn_f'] = np.nan\n",
    "\n",
    "    if 'channel_type' in df.columns:\n",
    "        # normalize channel_type as string categories\n",
    "        df['channel_type_norm'] = df['channel_type'].astype(str).str.strip().fillna('UNK')\n",
    "    else:\n",
    "        df['channel_type_norm'] = 'UNK'\n",
    "\n",
    "    if 'currency_type' in df.columns:\n",
    "        df['currency_type_norm'] = df['currency_type'].astype(str).str.strip().fillna('UNK')\n",
    "    else:\n",
    "        df['currency_type_norm'] = 'UNK'\n",
    "\n",
    "    # counterparties\n",
    "    if not set(['from_acct', 'to_acct']).issubset(df.columns):\n",
    "        raise ValueError(\"from_acct and to_acct must exist in transaction data.\")\n",
    "\n",
    "    # --- per-account aggregates (sender side) ---\n",
    "    g_from = df.groupby('from_acct')\n",
    "    send_amt_sum = g_from['txn_amt'].sum().rename('send_amt_sum')\n",
    "    send_amt_mean = g_from['txn_amt'].mean().rename('send_amt_mean')\n",
    "    send_amt_std = g_from['txn_amt'].std(ddof=0).fillna(0).rename('send_amt_std')\n",
    "    send_amt_max = g_from['txn_amt'].max().rename('send_amt_max')\n",
    "    send_amt_min = g_from['txn_amt'].min().rename('send_amt_min')\n",
    "    send_amt_median = g_from['txn_amt'].median().rename('send_amt_median')\n",
    "    send_cnt = g_from.size().rename('send_cnt')\n",
    "\n",
    "    # unique counterparties sent to, reciprocity later\n",
    "    send_unique_ctp = g_from['to_acct'].nunique().rename('send_unique_ctp')\n",
    "\n",
    "    # hours, channels, currency, self-txn ratio on sender side\n",
    "    send_night = g_from['hour'].apply(lambda s: ((s>=22) | (s<=6)).sum()).rename('send_night_cnt')\n",
    "    send_hour_mean = g_from['hour'].mean().rename('send_hour_mean')\n",
    "    send_hour_std = g_from['hour'].std(ddof=0).fillna(0).rename('send_hour_std')\n",
    "\n",
    "    send_self_cnt = g_from['is_self_txn_f'].sum(min_count=1).fillna(0).rename('send_self_cnt')\n",
    "    send_self_ratio = (send_self_cnt / send_cnt.replace(0, np.nan)).fillna(0).rename('send_self_ratio')\n",
    "\n",
    "    # channel one-hot proportions (top categories)\n",
    "    top_channels = df['channel_type_norm'].value_counts().head(8).index.tolist()\n",
    "    send_channel_props = []\n",
    "    for ch in top_channels:\n",
    "        colname = f'send_ch_{ch}_prop'\n",
    "        send_channel_props.append(\n",
    "            g_from.apply(lambda s, ch=ch: (s['channel_type_norm']==ch).mean()).rename(colname)\n",
    "        )\n",
    "\n",
    "    # currency stats\n",
    "    send_curr_nunique = g_from['currency_type_norm'].nunique().rename('send_curr_nunique')\n",
    "    send_curr_twd_ratio = g_from.apply(lambda s: (s['currency_type_norm']=='TWD').mean()).rename('send_curr_twd_ratio')\n",
    "\n",
    "    # esun-to-others ratio when sending\n",
    "    send_to_others_ratio = g_from['to_acct_type'].mean().rename('send_to_esun_ratio')  # 1 means esun, 0 others\n",
    "    # we also keep 1 - ratio (others), but model can infer\n",
    "\n",
    "    # recency & activity\n",
    "    send_active_days = g_from['txn_date_dt'].nunique().rename('send_active_days')\n",
    "    ref_date = df['txn_date_dt'].max()\n",
    "    send_last_gap_days = g_from['txn_date_dt'].max().apply(lambda t: (ref_date - t).days if pd.notna(t) else np.nan).rename('send_last_gap_days')\n",
    "    send_avg_amt_per_day = (send_amt_sum / send_active_days.replace(0, np.nan)).fillna(0).rename('send_avg_amt_per_day')\n",
    "\n",
    "    # --- per-account aggregates (receiver side) ---\n",
    "    g_to = df.groupby('to_acct')\n",
    "    recv_amt_sum = g_to['txn_amt'].sum().rename('recv_amt_sum')\n",
    "    recv_amt_mean = g_to['txn_amt'].mean().rename('recv_amt_mean')\n",
    "    recv_amt_std = g_to['txn_amt'].std(ddof=0).fillna(0).rename('recv_amt_std')\n",
    "    recv_amt_max = g_to['txn_amt'].max().rename('recv_amt_max')\n",
    "    recv_amt_min = g_to['txn_amt'].min().rename('recv_amt_min')\n",
    "    recv_amt_median = g_to['txn_amt'].median().rename('recv_amt_median')\n",
    "    recv_cnt = g_to.size().rename('recv_cnt')\n",
    "    recv_unique_ctp = g_to['from_acct'].nunique().rename('recv_unique_ctp')\n",
    "\n",
    "    recv_night = g_to['hour'].apply(lambda s: ((s>=22) | (s<=6)).sum()).rename('recv_night_cnt')\n",
    "    recv_hour_mean = g_to['hour'].mean().rename('recv_hour_mean')\n",
    "    recv_hour_std = g_to['hour'].std(ddof=0).fillna(0).rename('recv_hour_std')\n",
    "\n",
    "    recv_self_cnt = g_to['is_self_txn_f'].sum(min_count=1).fillna(0).rename('recv_self_cnt')\n",
    "    recv_self_ratio = (recv_self_cnt / recv_cnt.replace(0, np.nan)).fillna(0).rename('recv_self_ratio')\n",
    "\n",
    "    recv_channel_props = []\n",
    "    for ch in top_channels:\n",
    "        colname = f'recv_ch_{ch}_prop'\n",
    "        recv_channel_props.append(\n",
    "            g_to.apply(lambda s, ch=ch: (s['channel_type_norm']==ch).mean()).rename(colname)\n",
    "        )\n",
    "\n",
    "    recv_curr_nunique = g_to['currency_type_norm'].nunique().rename('recv_curr_nunique')\n",
    "    recv_curr_twd_ratio = g_to.apply(lambda s: (s['currency_type_norm']=='TWD').mean()).rename('recv_curr_twd_ratio')\n",
    "\n",
    "    recv_from_esun_ratio = g_to['from_acct_type'].mean().rename('recv_from_esun_ratio')\n",
    "\n",
    "    recv_active_days = g_to['txn_date_dt'].nunique().rename('recv_active_days')\n",
    "    recv_last_gap_days = g_to['txn_date_dt'].max().apply(lambda t: (ref_date - t).days if pd.notna(t) else np.nan).rename('recv_last_gap_days')\n",
    "    recv_avg_amt_per_day = (recv_amt_sum / recv_active_days.replace(0, np.nan)).fillna(0).rename('recv_avg_amt_per_day')\n",
    "\n",
    "    # --- graph-like reciprocity ---\n",
    "    # For each account, proportion of counterparties with two-way flows\n",
    "    # Build adjacency sets\n",
    "    out_sets = df.groupby('from_acct')['to_acct'].apply(set)\n",
    "    in_sets = df.groupby('to_acct')['from_acct'].apply(set)\n",
    "    all_accts = set(out_sets.index).union(set(in_sets.index))\n",
    "\n",
    "    reci_ratio = {}\n",
    "    total_degree = {}\n",
    "    bi_degree = {}\n",
    "\n",
    "    for a in all_accts:\n",
    "        outs = out_sets.get(a, set())\n",
    "        ins = in_sets.get(a, set())\n",
    "        deg = len(outs.union(ins))\n",
    "        bi = len(outs.intersection(ins))\n",
    "        total_degree[a] = deg\n",
    "        bi_degree[a] = bi\n",
    "        reci_ratio[a] = _safe_div(bi, deg)\n",
    "\n",
    "    df_graph = pd.DataFrame({\n",
    "        'acct': list(all_accts),\n",
    "        'graph_degree': [total_degree[a] for a in all_accts],\n",
    "        'graph_bi_degree': [bi_degree[a] for a in all_accts],\n",
    "        'graph_reciprocity': [reci_ratio[a] for a in all_accts],\n",
    "    })\n",
    "\n",
    "    # --- merge all features to account level ---\n",
    "    # Sender features -> index is 'from_acct'\n",
    "    left = (\n",
    "        pd.concat([\n",
    "            send_amt_sum, send_amt_mean, send_amt_std, send_amt_max, send_amt_min, send_amt_median, send_cnt,\n",
    "            send_unique_ctp, send_night, send_hour_mean, send_hour_std, send_self_cnt, send_self_ratio,\n",
    "            send_curr_nunique, send_curr_twd_ratio, send_to_others_ratio, send_active_days,\n",
    "            send_last_gap_days, send_avg_amt_per_day\n",
    "        ] + send_channel_props, axis=1)\n",
    "        .reset_index().rename(columns={'from_acct': 'acct'})\n",
    "    )\n",
    "\n",
    "    # Receiver features -> index is 'to_acct'\n",
    "    right = (\n",
    "        pd.concat([\n",
    "            recv_amt_sum, recv_amt_mean, recv_amt_std, recv_amt_max, recv_amt_min, recv_amt_median, recv_cnt,\n",
    "            recv_unique_ctp, recv_night, recv_hour_mean, recv_hour_std, recv_self_cnt, recv_self_ratio,\n",
    "            recv_curr_nunique, recv_curr_twd_ratio, recv_from_esun_ratio, recv_active_days,\n",
    "            recv_last_gap_days, recv_avg_amt_per_day\n",
    "        ] + recv_channel_props, axis=1)\n",
    "        .reset_index().rename(columns={'to_acct': 'acct'})\n",
    "    )\n",
    "\n",
    "    # Outer merge sender/receiver features\n",
    "    feat = pd.merge(left, right, on='acct', how='outer')\n",
    "\n",
    "    # Add simple totals / balances\n",
    "    feat['total_amt_sum'] = feat['send_amt_sum'].fillna(0) + feat['recv_amt_sum'].fillna(0)\n",
    "    feat['net_out_amt'] = feat['send_amt_sum'].fillna(0) - feat['recv_amt_sum'].fillna(0)\n",
    "    feat['total_cnt'] = feat['send_cnt'].fillna(0) + feat['recv_cnt'].fillna(0)\n",
    "\n",
    "    # Normalize some ratios\n",
    "    feat['send_avg_amt'] = _safe_div(feat['send_amt_sum'].fillna(0), feat['send_cnt'].replace(0, np.nan))\n",
    "    feat['recv_avg_amt'] = _safe_div(feat['recv_amt_sum'].fillna(0), feat['recv_cnt'].replace(0, np.nan))\n",
    "    feat['send_unique_rate'] = _safe_div(feat['send_unique_ctp'].fillna(0), feat['send_cnt'].replace(0, np.nan))\n",
    "    feat['recv_unique_rate'] = _safe_div(feat['recv_unique_ctp'].fillna(0), feat['recv_cnt'].replace(0, np.nan))\n",
    "\n",
    "    # Merge graph features\n",
    "    feat = feat.merge(df_graph, on='acct', how='left')\n",
    "\n",
    "    # Determine account type (if account ever appears as from_acct or to_acct with acct_type)\n",
    "    df_from_type = df[['from_acct', 'from_acct_type']].drop_duplicates().rename(columns={'from_acct': 'acct', 'from_acct_type': 'is_esun_from'})\n",
    "    df_to_type = df[['to_acct', 'to_acct_type']].drop_duplicates().rename(columns={'to_acct': 'acct', 'to_acct_type': 'is_esun_to'})\n",
    "    feat = feat.merge(df_from_type, on='acct', how='left').merge(df_to_type, on='acct', how='left')\n",
    "    feat['is_esun'] = feat[['is_esun_from', 'is_esun_to']].max(axis=1)\n",
    "\n",
    "    # Fill NaNs with 0 for model; keep acct id\n",
    "    feat = feat.fillna(0)\n",
    "    print(f\"[OK] Feature engineering completed. Accounts: {len(feat)}; Features: {feat.shape[1]-1}\")\n",
    "    return feat\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train / Validation / Test Split\n",
    "# -----------------------------\n",
    "def make_splits(feat_df: pd.DataFrame, df_alert: pd.DataFrame, df_test: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build training labels and test set.\n",
    "    - Label for training: acct in df_alert['acct'] -> 1 else 0\n",
    "    - Only use Esun accounts for training (per spec & to match test distribution)\n",
    "    - Exclude test accounts from training\n",
    "    \"\"\"\n",
    "    feat = feat_df.copy()\n",
    "\n",
    "    # Labels\n",
    "    alert_set = set(df_alert['acct'].astype(str))\n",
    "    feat['label'] = feat['acct'].astype(str).isin(alert_set).astype(int)\n",
    "\n",
    "    # Test mask\n",
    "    test_set = set(df_test['acct'].astype(str))\n",
    "\n",
    "    # Keep only esun in train\n",
    "    train_df = feat[(~feat['acct'].astype(str).isin(test_set)) & (feat['is_esun'] == 1)].copy()\n",
    "    X = train_df.drop(columns=['label'])\n",
    "    y = train_df['label'].values\n",
    "\n",
    "    # Test data: exactly test acct list joined with features (missing -> 0)\n",
    "    test_feat = feat[feat['acct'].astype(str).isin(test_set)].copy()\n",
    "    # Guarantee order same as df_test\n",
    "    X_test = df_test[['acct']].merge(test_feat.drop(columns=['label']), on='acct', how='left').fillna(0)\n",
    "\n",
    "    print(f\"[OK] Split -> Train accounts: {len(X)} (pos={sum(y)}, neg={len(y)-sum(y)}); Test accounts: {len(X_test)}\")\n",
    "    return X, y, X_test\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Modeling\n",
    "# -----------------------------\n",
    "def fit_model_and_tune_threshold(X: pd.DataFrame, y: np.ndarray, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Train RandomForest with reasonable defaults and tune decision threshold on a validation set\n",
    "    to maximize F1 (handles imbalance more sensibly).\n",
    "    \"\"\"\n",
    "    # Features to use: drop identifiers only\n",
    "    drop_cols = ['acct']\n",
    "    feat_cols = [c for c in X.columns if c not in drop_cols]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X[feat_cols], y, test_size=0.2, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=600,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced_subsample',\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Validation metrics & threshold tuning\n",
    "    val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "    ap = average_precision_score(y_val, val_proba)\n",
    "    auc = roc_auc_score(y_val, val_proba)\n",
    "\n",
    "    # pick threshold with best F1\n",
    "    prec, rec, thr = precision_recall_curve(y_val, val_proba)\n",
    "    f1s = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    best_idx = np.nanargmax(f1s)\n",
    "    best_thr = 0.5\n",
    "    # precision_recall_curve returns len(thr)=len(prec)-1; align safely\n",
    "    if best_idx < len(thr):\n",
    "        best_thr = thr[best_idx]\n",
    "    else:\n",
    "        # edge case: choose 0.5 if cannot align\n",
    "        best_thr = 0.5\n",
    "\n",
    "    y_val_pred = (val_proba >= best_thr).astype(int)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"[VAL] AP={ap:.4f}  AUC={auc:.4f}  F1@best={f1:.4f}  thr={best_thr:.4f}\")\n",
    "    print(\"[VAL] Classification report:\\n\", classification_report(y_val, y_val_pred, digits=4))\n",
    "    return clf, feat_cols, best_thr\n",
    "\n",
    "\n",
    "def predict_test(clf, feat_cols: List[str], threshold: float, X_test: pd.DataFrame):\n",
    "    proba = clf.predict_proba(X_test[feat_cols])[:, 1]\n",
    "    y_pred = (proba >= threshold).astype(int)\n",
    "    return y_pred, proba\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Output\n",
    "# -----------------------------\n",
    "def save_submission(path: str, df_test: pd.DataFrame, X_test: pd.DataFrame, y_pred: np.ndarray):\n",
    "    df_pred = pd.DataFrame({\n",
    "        'acct': X_test['acct'].values,\n",
    "        'label': y_pred.astype(int)\n",
    "    })\n",
    "    out = df_test[['acct']].merge(df_pred, on='acct', how='left').fillna(0)\n",
    "    out.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK] Saved submission to: {path}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # === change this to your data directory ===\n",
    "    dir_path = \"data\"\n",
    "\n",
    "    df_txn, df_alert, df_test = load_csvs(dir_path)\n",
    "    feat_df = engineer_features(df_txn)\n",
    "    X, y, X_test = make_splits(feat_df, df_alert, df_test)\n",
    "    clf, feat_cols, thr = fit_model_and_tune_threshold(X, y, random_state=42)\n",
    "    y_pred, _ = predict_test(clf, feat_cols, thr, X_test)\n",
    "    save_submission(\"result.csv\", df_test, X_test, y_pred)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab1-Exercise (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
